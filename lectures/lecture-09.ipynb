{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2eead98-dadd-491c-858b-25678d48dc7c",
   "metadata": {},
   "source": [
    "# Our Class Plan in One Big Graph\n",
    "\n",
    "![](../images/ml.png)\n",
    "\n",
    "# Ensemble Learning\n",
    "\n",
    "Ensemble learning is a type of machine learning where the learning algorithm combines multiple models to improve its predictive performance. There are two main techniques within ensemble learning: bagging and boosting.\n",
    "\n",
    "## Combining different random variables\n",
    "\n",
    "When we combine different random variables into an ensemble, the resulting ensemble may exhibit a lower variance than each random variable. This phenomenon arises due to the correlation or covariance structure among the individual random variables.\n",
    "\n",
    "Recall that *covariance* measures the degree to which two random variables change together. If the covariance between two variables is positive, they tend to increase or decrease together; if it's negative, they tend to move in opposite directions. On the other hand, *correlation*, which ranges from -1 to 1, measures the linear relationship between random variables where 1 indicates a perfect positive linear relationship while -1 indicates a perfect negative linear relationship, and 0 indicates no linear relationship.\n",
    "\n",
    "When we combine different random variables where the individual random variables exhibit a low degree of correlation, or even negative covariance, the combined random variable may have reduced variability compared to the individual variables. When the individual random variables' predictions are not correlated, combining them tends to smooth out fluctuations, leading to reduced variability in the combined variable. If the individual variables' predictions are negatively correlated, combining them can cancel out some of the variability, again leading to reduced variability in the combined variable.\n",
    "\n",
    "## An analogy with portfolio design\n",
    "\n",
    "In finance, specifically in portfolio design theory, the goal is often to design a portfolio that achieves a desired level of return while minimizing the risk. The risk in this context is measured in terms of the variance of the returns. This is usually achieved via **portfolio diversification** where risk is reduced by combining different assets. By combining assets with uncorrelated or negatively correlated returns, investors potentially reduce the overall variance of the portfolio and lower the risk without sacrificing returns.\n",
    "\n",
    "Diversification involves holding a variety of investments in a portfolio that have different risk-return profiles.  When assets in a portfolio have low or negative correlations with each other, their returns tend to move independently or even in opposite directions over time.  As a result, when these assets are combined in a portfolio, the overall portfolio's variance or risk is reduced due to the offsetting effects of the individual asset returns.\\\\\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Bagging, also known as Bootstrap Aggregating, algorithms train multiple models independently (in most cases in parallel) on different subsets of the training data. The algorithms create these subsets by randomly sampling the training data **with replacement**.  It is important to note that each model in the ensemble is trained on a different subset.  Then the algorithm combines the predictions coming from each model through averaging (for regression models) or voting (for classification models). \n",
    "\n",
    "Bagging algorithms reduce the total variance by averaging over multiple noisy models, thereby improve overall prediction accuracy.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is an iterative ensemble method. Boosting algorithms train base learners **sequentially**. In each iteration each subsequent model focuses on the instances that the previous models failed to worked, or showed weak performance. To put another way, boosting models train each model to correct the errors made by its predecessors.  Such algorithms achieve this goal by assigning higher weights to the instances that were misclassified in the previous iterations.  In the final prediction, the result is obtained by weighted averaging where the weights are determined based on the performance of each model.\n",
    "\n",
    "## Similarities and Differences\n",
    "\n",
    "Now, let's delve into the differences and similarities between bagging and boosting:\n",
    "\n",
    "1. **Training Approach**:\n",
    " + Bagging: Models are trained independently on different subsets of the training data.\n",
    " + Boosting: Models are trained sequentially, with each subsequent model focusing on the instances that previous models have struggled with.\n",
    "\n",
    "\n",
    "2. **Weighting of Instances**:\n",
    "\n",
    " + Bagging: Each model is trained on a randomly sampled subset of the training data with replacement. All instances have equal weight.\n",
    " + Boosting: Instances are weighted based on their performance in previous iterations. Misclassified instances are given higher weight to be correctly classified in subsequent iterations.\n",
    "\n",
    "\n",
    "3. **Predictor Diversity**:\n",
    "\n",
    " + Bagging: Models in the ensemble are typically diverse since they are trained on different subsets of data.\n",
    " + Boosting: Models in the ensemble are sequential, and each subsequent model focuses on the mistakes of the previous ones, potentially leading to less diversity.\n",
    "\n",
    "\n",
    "4. **Model Combination**:\n",
    "\n",
    " + Bagging: Predictions are combined through averaging (for regression) or voting (for classification).\n",
    " + Boosting: Predictions are combined through weighted averaging, where the weights are determined based on the performance of each model.\n",
    "\n",
    "\n",
    "5. **Performance**:\n",
    "\n",
    " + Bagging: Often reduces variance and can help to alleviate overfitting, particularly for unstable models.\n",
    " + Boosting: Tends to produce models with low bias and low variance, which can lead to improved performance, particularly when used with weak learners.\n",
    "\n",
    "\n",
    "## Ensemble classification strategies\n",
    "\n",
    "In the One vs. Rest (OvR) strategy (also known as one-vs-all or one-against-all) the learning algorithm trains a separate binary classifier model for each class where each classifier is responsible for distinguishing one class from the rest. In the training stage, the data points that belong to a chosen class are considered as *positive* while all other data points are considered as *negative*. In the prediction stage, each classifier produces a probability indicating whether the test point belongs to a particular class.  The final prediction is determined by the highest probability.\n",
    "\n",
    "In the One-vs-One (OvO) strategy, also known as all-pairs or pairwise classification, the algorithm trains a binary classifier for every pair of classes. Thus for a multiclass classification problem with $k$ classes, we would need $\\frac{k(k-1)}{2}$ binary classifiers. Each classifier is then trained on a subset of the training set that contains instances from only those two classes where one class is considered positive while the other is negative.  During prediction, each binary classifier produces a binary prediction indicating which of the two classes the instance belongs to.  The final predicted class is determined based on voting where each class *votes* for its class label based on the majority of votes from all binary classifiers.\n",
    "\n",
    "OvR is typically more computationally efficient than OvO since it requires training fewer binary classifiers. However, OvO may lead to more balanced training datasets for binary classifiers, especially in cases where the number of instances per class is imbalanced.  OvR tends to perform better in practice when the number of classes is large, while OvO may be preferable when the number of classes is small or when the binary classifiers' training is not too expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0e721-4cd8-43bd-a821-dedbfba7efc9",
   "metadata": {},
   "source": [
    "## Random Forest:\n",
    "\n",
    "Random Forest is one of the most widely used bagging algorithms, particularly for classification and regression tasks. It consists of an ensemble of decision trees, where each tree is trained on a bootstrap sample of the training data and makes predictions independently.  During the training process, at each node of the tree, a random subset of features is considered for splitting, leading to decorrelated trees.  The final prediction is obtained by averaging (regression) or voting (classification) over all trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e799951-fdae-41d2-80db-55b6cb73e5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "107993a7-318a-4712-8223-657cc59475d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(X,y,strategy,model,test=0.25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test)\n",
    "    res = strategy(model).fit(X_train, y_train)\n",
    "    y_pred = res.predict(X_test)\n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eff095-35d2-4ccc-85b0-eb7933074ba0",
   "metadata": {},
   "source": [
    "### IRIS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e63bf94e-af3f-44cd-b418-2b53e8f9d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a2a3a7d6-69f8-4741-856f-2729b9d9dd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         8\n",
      "           1       0.87      0.93      0.90        14\n",
      "           2       0.93      0.88      0.90        16\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.93      0.93        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(iris_X, iris_y,OneVsRestClassifier,RandomForestClassifier(n_estimators=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "017c2139-f403-4c50-b31f-b3bace754452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.90      1.00      0.95         9\n",
      "           2       1.00      0.94      0.97        18\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(iris_X, iris_y,OneVsOneClassifier,RandomForestClassifier(n_estimators=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "97997c92-89d5-4c9b-a5df-a3c35608f02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.92      0.85      0.88        13\n",
      "           2       0.87      0.93      0.90        14\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.93      0.92      0.93        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(iris_X, iris_y,OneVsRestClassifier,LogisticRegression(max_iter=3000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d8fb6a67-801a-4208-b1c6-39b321831f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.93      1.00      0.96        13\n",
      "           2       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.98      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(iris_X, iris_y, OneVsOneClassifier, SVC()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d848be-00f7-410f-ae1b-2d020148695e",
   "metadata": {},
   "source": [
    "### Digits Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f2fee0d8-af5b-4798-bd29-e45fbaef3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits_X = digits.data\n",
    "digits_y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d1d89b4a-8418-4847-9429-7649f7a8c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        39\n",
      "           1       0.98      0.95      0.97        44\n",
      "           2       1.00      1.00      1.00        49\n",
      "           3       0.95      0.93      0.94        44\n",
      "           4       0.98      0.90      0.94        51\n",
      "           5       0.93      0.93      0.93        46\n",
      "           6       0.98      1.00      0.99        43\n",
      "           7       0.94      0.98      0.96        49\n",
      "           8       1.00      0.89      0.94        46\n",
      "           9       0.81      0.97      0.88        39\n",
      "\n",
      "    accuracy                           0.96       450\n",
      "   macro avg       0.96      0.96      0.96       450\n",
      "weighted avg       0.96      0.96      0.96       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsRestClassifier, RandomForestClassifier(n_estimators=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2b4ebb8a-38b1-4c11-aadd-3b56bb27c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        42\n",
      "           1       0.94      0.98      0.96        48\n",
      "           2       0.98      0.96      0.97        55\n",
      "           3       0.94      0.94      0.94        50\n",
      "           4       0.95      1.00      0.98        40\n",
      "           5       0.98      0.95      0.96        42\n",
      "           6       1.00      0.98      0.99        43\n",
      "           7       0.98      1.00      0.99        40\n",
      "           8       0.94      0.84      0.88        55\n",
      "           9       0.87      0.97      0.92        35\n",
      "\n",
      "    accuracy                           0.96       450\n",
      "   macro avg       0.96      0.96      0.96       450\n",
      "weighted avg       0.96      0.96      0.96       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsRestClassifier, LogisticRegression(max_iter=4000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1d5d9483-1f6e-436d-af0a-550b37d086d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        44\n",
      "           1       0.98      1.00      0.99        43\n",
      "           2       1.00      1.00      1.00        42\n",
      "           3       1.00      1.00      1.00        49\n",
      "           4       0.98      0.95      0.96        42\n",
      "           5       1.00      1.00      1.00        45\n",
      "           6       1.00      1.00      1.00        48\n",
      "           7       1.00      1.00      1.00        40\n",
      "           8       0.98      1.00      0.99        42\n",
      "           9       0.98      0.98      0.98        55\n",
      "\n",
      "    accuracy                           0.99       450\n",
      "   macro avg       0.99      0.99      0.99       450\n",
      "weighted avg       0.99      0.99      0.99       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsOneClassifier, SVC()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84676967-c0de-4a82-a0c1-d27e4875fc13",
   "metadata": {},
   "source": [
    "## ADABoost\n",
    "\n",
    "AdaBoost is an ensemble learning algorithm that combines multiple weak models to create a stronger model. AdaBoost algorithm trains weaker models (models that perform slightly better than random guessing) iteratively on weighted training data.  It assigns higher weights to points misclassified by the previous models and thus focuses on more difficult data points in each iteration. It achieves a final prediction by combining all predictions through a weighted sum.\n",
    "\n",
    "### Training\n",
    "\n",
    "AdaBoost trains a sequence of weak models where each subsequent model focuses on the mistakes made by the previous ones.  At each iteration, a new model is trained on a modified version of the training data, where the weights of instances are adjusted based on their classification errors.  Even though AdaBoost can use any machine learning algorithm as its base estimator, decision trees with a single node are a common choice.  Decision stumps are simple classifiers that make decisions based on a single feature, making them computationally efficient and less prone to overfitting.\n",
    "\n",
    "### Assigning weights to data points\n",
    "\n",
    "At the beginning, the points in the train set are assigned equal weights. In each iteration, the algorithm simultaneously increases the weights of misclassified points while decreasing the weights for correctly classified points. This allows AdaBoost to focus more on difficult-to-classify data points. This improves the overall performance of the ensemble.\n",
    "\n",
    "### Stopping and the final prediction\n",
    "\n",
    "After training a predefined number of models, or until a specified stopping criterion is met, AdaBoost combines all predictions for a final prediction by taking a weighted sum of these predictions. The weights (called *boosting factors*) are determined based on the accuracy of each weak learner.  The boosting factor for each model is determined by the accuracy, with more accurate models having a higher influence on the final prediction.\n",
    "\n",
    "AdaBoost can be adapted for both binary classification and multi-class classification tasks.  For binary classification tasks, AdaBoost typically returns class probabilities that can be transformed into binary predictions using a specified threshold.  For multi-class classification tasks, AdaBoost uses strategies like One-vs-Rest (OvR) or One-vs-One (OvO) to extend the algorithm to handle multiple classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b3296914-12c3-4840-b226-5f7882493a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        38\n",
      "   macro avg       1.00      1.00      1.00        38\n",
      "weighted avg       1.00      1.00      1.00        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(iris_X,iris_y,OneVsOneClassifier,AdaBoostClassifier(estimator=LogisticRegression(max_iter=2000)),test=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "7494e3b0-fe53-4906-91f9-d7dbdc80be6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        42\n",
      "           1       0.92      0.98      0.95        49\n",
      "           2       1.00      1.00      1.00        37\n",
      "           3       0.98      0.96      0.97        45\n",
      "           4       0.98      0.98      0.98        52\n",
      "           5       0.93      0.95      0.94        42\n",
      "           6       1.00      0.98      0.99        41\n",
      "           7       0.98      1.00      0.99        42\n",
      "           8       0.91      0.86      0.89        50\n",
      "           9       0.96      0.98      0.97        50\n",
      "\n",
      "    accuracy                           0.96       450\n",
      "   macro avg       0.97      0.97      0.97       450\n",
      "weighted avg       0.96      0.96      0.96       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsOneClassifier, AdaBoostClassifier(algorithm='SAMME'),test=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "54cad021-c549-4b63-8537-9bfae3f6514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        52\n",
      "           1       0.93      0.95      0.94        43\n",
      "           2       1.00      0.95      0.98        43\n",
      "           3       0.94      0.89      0.92        37\n",
      "           4       1.00      0.93      0.96        57\n",
      "           5       0.89      0.95      0.92        42\n",
      "           6       0.97      0.97      0.97        37\n",
      "           7       0.91      0.96      0.94        53\n",
      "           8       0.92      0.97      0.94        35\n",
      "           9       0.94      0.90      0.92        51\n",
      "\n",
      "    accuracy                           0.95       450\n",
      "   macro avg       0.95      0.95      0.95       450\n",
      "weighted avg       0.95      0.95      0.95       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsOneClassifier, AdaBoostClassifier(algorithm='SAMME'),test=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "65ac63d9-aead-4b6e-94e3-1bed3e099c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        45\n",
      "           1       0.00      0.00      0.00        44\n",
      "           2       0.00      0.00      0.00        46\n",
      "           3       0.00      0.00      0.00        57\n",
      "           4       0.00      0.00      0.00        50\n",
      "           5       0.10      1.00      0.18        45\n",
      "           6       0.00      0.00      0.00        40\n",
      "           7       0.00      0.00      0.00        42\n",
      "           8       0.00      0.00      0.00        42\n",
      "           9       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.10       450\n",
      "   macro avg       0.01      0.10      0.02       450\n",
      "weighted avg       0.01      0.10      0.02       450\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsRestClassifier, AdaBoostClassifier(estimator=SVC(max_iter=2000, probability=True), algorithm='SAMME'),test=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36847023-638b-4eb3-a0fb-a4f783f93f60",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "\n",
    "LightGBM is a gradient-boosting framework known for its efficiency, scalability, and high performance. Like any other ensemble algorithm, it iteratively combines weaker models (typically decision trees) to create a stronger model. It builds decision trees sequentially where each new tree is trained to correct the errors made by previous trees. The algorithm is widely used for supervised learning tasks such as classification and regression problems. \n",
    "\n",
    "\n",
    "A distinguishing feature of LightGBM is that it uses a histogram-based algorithm for efficient tree construction. Instead of using exact algorithms for finding the best-split points, LightGBM discretizes continuous features into discrete bins (histograms) and uses these histograms to find approximate split points. Therefore, it natively supports categorical features without the need for one-hot encoding.  Histogram-based splitting reduces memory consumption and computation time, making LightGBM highly efficient, especially on large datasets.\n",
    "\n",
    "LightGBM grows trees leaf-wise, instead of the traditional level-wise approach used in other boosting algorithms. In this approach, the algorithm grows the tree by splitting the leaf giving the largest decrease in the loss function. This leads to deeper trees with fewer nodes, allowing the algorithm to capture complex patterns more efficiently.\n",
    "\n",
    "LightGBM uses gradient-based optimization techniques to train each tree in the ensemble.  It minimizes a differentiable loss function by iteratively updating the tree structure to move in the direction of the negative gradient of the loss function. This gradient-based approach enables LightGBM to efficiently optimize complex objective functions, including custom loss functions.\n",
    "\n",
    "\n",
    "LightGBM is designed for parallel and distributed computing, allowing it to scale efficiently to large datasets and distributed computing environments. It supports multi-threading for parallel training on multi-core CPUs and distributed training on distributed computing frameworks such as Apache Spark. Due to its efficient algorithms and optimization techniques, LightGBM is known for its high performance and scalability. It is often the algorithm of choice for large-scale machine learning tasks, such as those involving high-dimensional datasets or massive amounts of data. \n",
    "\n",
    "\n",
    "LightGBM provides various regularization techniques to prevent overfitting and improve generalization performance.  Regularization parameters, such as `max_depth`, `min_child_samples`, and `lambda` (L2 regularization), can be tuned to control the complexity of the trees and reduce overfitting.\n",
    "\n",
    "For further information, read the [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "33a9792a-0d66-46f0-804d-79124a765ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 132, number of negative: 1215\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097996 -> initscore=-2.219697\n",
      "[LightGBM] [Info] Start training from score -2.219697\n",
      "[LightGBM] [Info] Number of positive: 130, number of negative: 1217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096511 -> initscore=-2.236610\n",
      "[LightGBM] [Info] Start training from score -2.236610\n",
      "[LightGBM] [Info] Number of positive: 139, number of negative: 1208\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103192 -> initscore=-2.162247\n",
      "[LightGBM] [Info] Start training from score -2.162247\n",
      "[LightGBM] [Info] Number of positive: 128, number of negative: 1219\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.095026 -> initscore=-2.253756\n",
      "[LightGBM] [Info] Start training from score -2.253756\n",
      "[LightGBM] [Info] Number of positive: 139, number of negative: 1208\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.103192 -> initscore=-2.162247\n",
      "[LightGBM] [Info] Start training from score -2.162247\n",
      "[LightGBM] [Info] Number of positive: 136, number of negative: 1211\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100965 -> initscore=-2.186547\n",
      "[LightGBM] [Info] Start training from score -2.186547\n",
      "[LightGBM] [Info] Number of positive: 130, number of negative: 1217\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.096511 -> initscore=-2.236610\n",
      "[LightGBM] [Info] Start training from score -2.236610\n",
      "[LightGBM] [Info] Number of positive: 143, number of negative: 1204\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.106162 -> initscore=-2.130560\n",
      "[LightGBM] [Info] Start training from score -2.130560\n",
      "[LightGBM] [Info] Number of positive: 134, number of negative: 1213\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.099480 -> initscore=-2.203012\n",
      "[LightGBM] [Info] Start training from score -2.203012\n",
      "[LightGBM] [Info] Number of positive: 136, number of negative: 1211\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 834\n",
      "[LightGBM] [Info] Number of data points in the train set: 1347, number of used features: 53\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100965 -> initscore=-2.186547\n",
      "[LightGBM] [Info] Start training from score -2.186547\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        46\n",
      "           1       0.92      0.94      0.93        52\n",
      "           2       0.97      0.97      0.97        38\n",
      "           3       0.98      0.98      0.98        55\n",
      "           4       0.97      0.93      0.95        42\n",
      "           5       1.00      0.98      0.99        46\n",
      "           6       1.00      0.98      0.99        51\n",
      "           7       0.95      1.00      0.97        36\n",
      "           8       0.97      0.93      0.95        40\n",
      "           9       0.91      0.98      0.95        44\n",
      "\n",
      "    accuracy                           0.97       450\n",
      "   macro avg       0.97      0.97      0.97       450\n",
      "weighted avg       0.97      0.97      0.97       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsRestClassifier, LGBMClassifier(num_leaves=10, n_estimators=50),test=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f33ef-7111-4bbe-a326-37951fff483b",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "XGBoost, short for Extreme Gradient Boosting, belongs to the family of gradient boosting algorithms.  XGBoost is a powerful and efficient gradient-boosting framework widely used for supervised learning tasks, particularly for classification, regression, and ranking problems. XGBoost employs a tree-boosting algorithm, where each tree is added to the ensemble sequentially to correct the errors made by the existing ensemble. XGBoost also uses techniques such as tree pruning and split finding to control the complexity of the trees and improve computational efficiency. It starts with an initial prediction (often the mean value for regression or the class with the highest frequency for classification) and iteratively adds trees to minimize the loss function. It uses an efficient algorithm to find the optimal split points for each feature, considering the information gain and the regularization term. \n",
    "\n",
    "XGBoost's regularized objective function consists of two main components: a loss function and a regularization term. The loss function measures the difference between the predicted and actual values, while the regularization term penalizes the complexity of the model to prevent overfitting. It minimizes the loss function by iteratively updating the model parameters (e.g., tree structure, leaf weights) in the direction of the negative gradient of the loss function. Common loss functions include squared loss for regression and softmax for multiclass classification.  Regularization parameters, such as `max_depth`, `min_child_weight`, and `gamma`, can be tuned to control the complexity of the trees and the regularization strength.\n",
    "\n",
    "XGBoost can handle missing values in the dataset by learning the optimal direction to handle missing values during training, thus eliminating the need for imputation or preprocessing steps.  XGBoost is also designed for parallel and distributed computing, allowing it to scale efficiently to large datasets and distributed computing environments. It supports multi-threading for parallel training on multi-core CPUs and distributed training on distributed computing frameworks.  It often outperforms other gradient-boosting frameworks and is widely used in machine-learning competitions and real-world applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "39fd7fed-a7ef-4df5-b299-d6ce3af23105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentXG(X,y,num_classes,rounds=50,test=0.25):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test)\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',  # Multiclass classification objective\n",
    "        'num_class': num_classes,  # Number of classes in the dataset\n",
    "        'eval_metric': 'merror'  # Evaluation metric: multiclass classification error rate\n",
    "    }\n",
    "    model = xgb.train(params, dtrain, rounds)\n",
    "    y_pred = model.predict(dtest)\n",
    "    \n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a387b0-fb51-4a01-986e-27e80ecfb34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d69f4f16-c191-40b0-94b2-08cb70dd9360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        44\n",
      "           1       1.00      0.94      0.97        47\n",
      "           2       1.00      0.94      0.97        52\n",
      "           3       0.93      0.93      0.93        44\n",
      "           4       0.92      0.92      0.92        48\n",
      "           5       0.90      0.95      0.93        40\n",
      "           6       0.98      0.98      0.98        42\n",
      "           7       0.94      0.98      0.96        50\n",
      "           8       0.95      0.98      0.96        42\n",
      "           9       0.93      0.93      0.93        41\n",
      "\n",
      "    accuracy                           0.95       450\n",
      "   macro avg       0.95      0.95      0.95       450\n",
      "weighted avg       0.95      0.95      0.95       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(experimentXG(digits_X,digits_y,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75627038-7288-40a5-b5f8-a3a6f01f1a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e67fd4-15d0-45f8-b5e2-292eee6d344d",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Assume we have a function $f\\colon \\Omega\\to \\mathbb{R}$ where $\\Omega$ is a compact (closed and bounded) set in $\\mathbb{R}^n$. How do we find its extremum (max or min) points? \n",
    "\n",
    "Remember that most machine learning algorithms are designed to optimize a loss, a reward, or a penalty function. Thus optimizers play a crucial role in training machine learning models by adjusting the model parameters (e.g., weights and biases) to minimize a predefined loss function. \n",
    "\n",
    "Here are some common optimizer choices:\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is the most fundamental optimization algorithm. It iteratively moves in the direction of the negative gradient of the loss function.\n",
    "\n",
    "$$ x_{n+1} = x_n - \\epsilon (\\nabla f)(x_n) $$\n",
    "\n",
    "Variants include:\n",
    "- **Stochastic Gradient Descent (SGD)**: Updates parameters using the gradient of the loss computed on a single randomly chosen training instance. It's computationally efficient but may have high variance in parameter updates.\n",
    "- **Mini-batch Gradient Descent**: Updates parameters using the gradient of the loss computed on a small batch of training instances. It strikes a balance between the efficiency of SGD and the stability of full-batch Gradient Descent.\n",
    "\n",
    "### RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "RMSprop is an adaptive optimization algorithm that adjusts the learning rates for each parameter based on the magnitude of recent gradients.  It divides the learning rate by a running average of the squared gradients, which helps to prevent the learning rate from decaying too quickly or too slowly. \n",
    "\n",
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam is an adaptive optimization algorithm that combines the benefits of both momentum and RMSprop. It maintains per-parameter learning rates based on estimates of the first and second moments of the gradients. Adam is widely used in deep learning and is known for its fast convergence and robustness to noisy gradients.\n",
    "\n",
    "### Adagrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "Adagrad is an adaptive optimization algorithm that adjusts the learning rates for each parameter that decreases over time as the sum of squared gradients accumulates. Adagrad is suitable for sparse data or problems with large differences in feature scales but may suffer from a diminishing learning rate problem.\n",
    "\n",
    "### AdaDelta\n",
    "\n",
    "AdaDelta is an extension of Adagrad that addresses the diminishing learning rate problem by using a decaying average of the learning rates based on a moving window of the past gradients without explicitly maintaining learning rates. \n",
    "\n",
    "### AdamW\n",
    "\n",
    "AdamW is a variant of Adam that introduces weight decay regularization to stabilize training and prevent overfitting.  It decouples weight decay from the optimization step, leading to more stable training dynamics and improved generalization performance. AdamW is particularly effective in training deep neural networks and has become popular in the deep learning community.\n",
    "\n",
    "### LBFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)\n",
    "\n",
    "LBFGS is a quasi-Newton optimization algorithm that approximates the Hessian matrix to perform line search optimization.  It maintains an estimate of the inverse Hessian matrix using limited memory, making it memory-efficient and suitable for problems with large numbers of parameters. LBFGS is often used in optimization problems with smooth and convex objective functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec37f21-1346-4235-a3f4-c64726f32607",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment(digits_X, digits_y, OneVsRestClassifier, LGBMClassifier(num_leaves=10, n_estimators=50),test=0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3769b0e7-f3b6-466c-94b9-4a482b604c95",
   "metadata": {},
   "source": [
    "## Yoklama\n",
    "\n",
    "- 090190328\n",
    "- 090200304\n",
    "- 090190315\n",
    "- 090200358\n",
    "- 090210362\n",
    "- 090210361\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
