{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation   \n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, LSTM, Embedding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,  classification_report\n",
    "from sklearn.datasets import load_iris, load_digits, fetch_20newsgroups_vectorized, fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH388E Lecture 10\n",
    "\n",
    "## Newton-Raphson Method\n",
    "\n",
    "The [Newton-Raphson algorithm](https://en.wikipedia.org/wiki/Newton%27s_method) is an iterative algorithm to find roots of a function. The main idea is to find successively better approximations to the roots. In order to find the roots, the algorithm starts with a random guess and then using the tangent line to the graph gets a better approximation. Then we repeat the procedure until we get a good-enough solution.\n",
    "\n",
    "Consider the following simple problem: assume we have a real valued function $f(x)$ and we would like to solve the equation \n",
    "\n",
    "$$ f(x) = c $$\n",
    "\n",
    "for some constant $c$.  Let us also assume that we made a guess $f(x_0) = c$.  Of course, unless we are extremely lucky, we are not going to hit the result. So, there will be an error:\n",
    "\n",
    "$$ f(x_0) = c + \\delta $$\n",
    "\n",
    "Now, using this error, let us improve our guess:\n",
    "\n",
    "$$ f(x_0) - \\delta = c $$\n",
    "\n",
    "But we want $\\delta$ to effect $x_0$.  Assuming we have a *local inverse* we get\n",
    "\n",
    "$$ f^{-1}(f(x_0) - \\delta) = f^{-1}(c) = a $$\n",
    "\n",
    "where $a$ is the solution we need to find.  Now, let us write the first order Taylor approximation for the left hand side:\n",
    "\n",
    "$$ x_0 - (f^{-1})'(x_0) \\cdot \\delta \\approx a $$\n",
    "\n",
    "and we know that $(f^{-1})'(x_0) = \\frac{1}{f'(x_0)}$\n",
    "\n",
    "So, our next best guess is going to be\n",
    "\n",
    "$$ x_1 = x_0 - \\frac{\\delta}{f'(x_0)} $$\n",
    "\n",
    "If we convert this formula into an iterative approximation, we get\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{\\delta_n}{f'(x_n)} $$\n",
    "\n",
    "where $\\delta_n = f(x_n) - c$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solve(f, c, x0, eta=1e-2, n=10000):\n",
    "    for i in range(n):\n",
    "        delta = f(x0) - c\n",
    "        der = (f(x0+eta/2) - f(x0-eta/2))/eta\n",
    "        x1 = x0-delta/(der+eta*np.random.rand())\n",
    "        if(abs(x0-x1)<eta):\n",
    "            break\n",
    "        else: \n",
    "            x0 = x1\n",
    "    return([i,x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    y = x*x\n",
    "    return(1.0 + math.cos(y+0.2)+math.log(0.24+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1.7734318154171977, 1.24]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Solve(fn,1.24,1.0,1e-10)\n",
    "[x[0],x[1],fn(x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The gradient descent\n",
    "\n",
    "Next, consider the following problem: we have a multivariable function $F(x_1,\\ldots,x_n)$ that we want to optimize, i.e. find the point at which $F$ attains its minimum or maximum. There is an iterative algorithm called [steepest descent algorithm](https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe21.pdf) similar to Newton-Raphson that we can use to find this point. The algorithm uses the [gradient](https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/def_gradient.html) of the function. Recall that the gradient $\\nabla F$ at a point $x$ \n",
    "\n",
    "$$ \\nabla F = \\left(\\frac{\\partial F}{\\partial x_1},\\ldots,\\frac{\\partial F}{\\partial x_n}\\right) $$\n",
    "\n",
    "gives us the direction at which $F$ has the largest (in absolute value) derivative. The algorithm uses this information and iteratively pushes an initial guess into better and better approximations of the optimum point.\n",
    "\n",
    "Let us start with an initial guess $a^{(0)}$ for $F(a_1^{(0)},\\ldots,a_n^{(0)}) = c$, the update rule is going to be\n",
    "\n",
    "$$ a^{(m+1)} = a^{(m)} - \\eta \\left(\\nabla F\\right)(a_1^{(m)},\\ldots,a_n^{(m)}) $$\n",
    "\n",
    "where $\\eta$ is called *the learning rate*.\n",
    "\n",
    "![](../images/steepest_descent.png)\n",
    "\n",
    "(Image is taken from [\"Learning-Based Auditory Encoding for Robust Speech Recognition\" by Yu-Hsiang Bosco Chiu, Bhiksha Raj, and Richard M Stern](https://www.researchgate.net/figure/An-example-of-steepest-descent-optimization-steps_fig2_220655581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f,x,eta=1e-4):\n",
    "    def delta(i,j): \n",
    "        if(i==j): return(1) \n",
    "        else: return(0)\n",
    "    def der(i,eta=1e-4):\n",
    "        vec = np.array([delta(i,j) for j in range(len(x))])\n",
    "        x1 = x + vec*eta/2\n",
    "        x0 = x - vec*eta/2\n",
    "        return((f(x1) - f(x0) + eta*np.random.rand())/eta)\n",
    "    return(np.array([der(i,eta) for i in range(len(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSolve(f,c,x0,eta=1e-4,n=1000):\n",
    "    for i in range(n):\n",
    "        delta = f(x0) - c\n",
    "        x1 = x0 - delta*eta*grad(f,x0,eta)\n",
    "        err = np.linalg.norm(x1-x0)\n",
    "        if(err < eta):\n",
    "            break\n",
    "        else:\n",
    "            x0 = x1\n",
    "    return([i,x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, array([0.00133204, 0.00123891])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x):\n",
    "    y = x[0]*x[0]+x[1]*x[1]\n",
    "    return(1.0+math.atan(y)+math.log(1.0+y))\n",
    "\n",
    "MSolve(g,3.0,[0.0,0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "\n",
    "\n",
    "Perceptrons are the main building blocks of artificial neural networks. They are designed to solve binary classification problems. They take a collection of input values $x = (x_1,\\ldots,x_n)$ apply a linear combination \n",
    "\n",
    "$$\\alpha\\cdot x + \\beta = a_1 x_1 + \\cdots + a_n x_n + \\beta$$ \n",
    "\n",
    "using a collection of weights $\\alpha = (a_0,\\ldots,a_n)$ and $\\beta$ to be determined via an iterative approach. Then we apply an activation function $\\varphi(x)$ to get an output which is either 0 or 1.\n",
    "\n",
    "![Perceptron](../images/perceptron.gif)\n",
    "\n",
    "([Source: Multilayer perceptrons from \"Nonlinear Switching State-Space Models\" by Antti Honkela](https://users.ics.aalto.fi/ahonkela/dippa/node41.html))\n",
    "\n",
    "\n",
    "This is a generalization of the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm we covered in earlier lectures.  In the logistic regression case $\\varphi(x) = \\frac{1}{1+e^{-x}}$.  So, if we have a collection of data points $(x^{(i)},y^{(i)})$ that we assume satisfy a relationship of the form\n",
    "\n",
    "$$ y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta) \\sim N(0,\\sigma) $$\n",
    "\n",
    "where $\\varphi\\colon\\mathbb{R}\\to\\mathbb{R}$ is a real valued function of a single variable, $\\alpha$ and $x^{(i)}$ are vectors in an inner product space and $\\beta$ is a scalar.  Our task is to find the best fitting pair $(\\alpha,\\beta)$ such that \n",
    "\n",
    "$$ \\sum_i (y^{(i)} - \\varphi(\\alpha\\cdot x^{(i)} + \\beta))^2 $$\n",
    "\n",
    "is minimized. So, we proceed by an iterative update:\n",
    "\n",
    "$$ \\alpha^{(n+1)} = \\alpha^{(n)} - \\frac{\\eta \\delta^{(n)}}{\\varphi'(\\alpha^{(n)}\\cdot x^{(n)}+\\beta^{(n)})} x^{(n)} $$\n",
    "\n",
    "where $\\delta^{(n)} = \\varphi(\\alpha^{(n)}\\cdot x^{(n)} + \\beta^{(n)}) - y^{(n)}$\n",
    "\n",
    "### Feed-forward and back-propagation\n",
    "\n",
    "In the feed-forward stage of the computation, we calculate the output $\\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$. In the back-propagation phase, we calculate the error $y - \\varphi(\\alpha^{(n)}\\cdot x + \\beta^{(n)})$ and adjust the weights as described above to obtain the next iteration of weights $(\\alpha^{(n+1)},\\beta^{(n+1)})$.\n",
    "\n",
    "### An example\n",
    "\n",
    "For this example, we are going to use a [toy dataset](http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)) from UCI: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.3513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "5  0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "6  0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
       "7  0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
       "8  0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
       "9  0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "5  0.3039  ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n",
       "6  0.3513  ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n",
       "7  0.2838  ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n",
       "8  0.1487  ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n",
       "9  0.0251  ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "5  0.0051  0.0062   R  \n",
       "6  0.0036  0.0103   R  \n",
       "7  0.0048  0.0053   R  \n",
       "8  0.0059  0.0022   R  \n",
       "9  0.0056  0.0040   R  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\",\n",
    "                    sep=\",\",\n",
    "                    header=None)\n",
    "sonar.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = sonar.iloc[:,0:60]\n",
    "ys = sonar.iloc[:,60].replace({'R': 0, 'M': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(f,x,eta):\n",
    "    return((f(x+eta/2)-f(x-eta/2))/eta)\n",
    "\n",
    "def sigmoid(x): return(1.0/(1.0+math.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xs, ys, f, epochs, batches, eta, tol):\n",
    "    n = len(xs.iloc[0,:])\n",
    "    num = len(xs)\n",
    "    w = np.random.rand(n+1)\n",
    "    err = []\n",
    "    temp = 0.0\n",
    "    for i in range(epochs):\n",
    "        j = np.random.randint(num)\n",
    "        x = xs.iloc[j,:]\n",
    "        y = ys[j]\n",
    "        x0 = np.append([1],x)\n",
    "        x1 = np.dot(w,x0)\n",
    "        delta = f(x1) - y\n",
    "        if(i%batches == batches-1):\n",
    "            err.append(temp)\n",
    "            temp = 0.0\n",
    "        elif(abs(delta) > tol):\n",
    "            temp = temp + 1.0/batches\n",
    "        der = diff(f,x1,eta)+eta*np.random.rand()\n",
    "        w = w - (der*delta*eta)*x0\n",
    "    return(ys,err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ee10a0e1e10>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict,errors = train(x_train,ys,sigmoid,int(1e4),500,5e-1,0.45)\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97,   0],\n",
       "       [  0, 111]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def helper(x,th=0.5):\n",
    "    if x>=th:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "tmp = np.vectorize(helper)(y_predict)\n",
    "confusion_matrix(ys,tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "While perceptrons are limited in their ability to learn complex non-linear relationships, they can be combined into more sophisticated architectures called [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network). Another way of saying would be a neural network is a directed computation graph of perceptions:\n",
    "\n",
    "![](../images/neural-net.png)\n",
    "\n",
    "(Source: [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network))\n",
    "\n",
    "Neural networks are composed of multiple layers of interconnected perceptrons. Thus their weights form a sequence of matrices (a tensor) $(w^i_{j,k})$.  In the feedforward phase of the network,  where the input $(x^i_j)$ at some layer $i$ is processed by the network as\n",
    "\n",
    "$$ x^{i+1}_k = \\sum_j f^i_k(x^i_j w^i_{j,k}) $$\n",
    "\n",
    "where $f^i_k$ is the activation function at the neuron $k$ at level $i$. Again, as in the case of perceptron, when we get the output from the output layer, we calculate the error and then propagate the error back updating weights iteratively. This procedure is a variation of the gradient descent algorithm we outlined above.\n",
    "\n",
    "There is a [very large number](http://www.asimovinstitute.org/neural-network-zoo/) of different types of neural networks. Unlike the perceptrons, it is neither practical nor recommended that you implement neural networks by hand. \n",
    "\n",
    "![](../images/meme.jpg)\n",
    "\n",
    " I would suggest Use one of the following libraries or frameworks: \n",
    "\n",
    "1. [TensorFlow](https://www.tensorflow.org/)\n",
    "2. [Keras](https://keras.io/)\n",
    "3. [scikit-learn Neural Network](https://github.com/aigamedev/scikit-neuralnetwork)\n",
    "4. [The Microsoft Cognitive Toolkit](https://learn.microsoft.com/en-us/cognitive-toolkit/)\n",
    "5. [Theano](https://github.com/Theano/Theano)\n",
    "6. [MXNet](https://mxnet.apache.org/versions/1.9.1/)\n",
    "\n",
    "Tensorflow has a very nice [playground](https://playground.tensorflow.org/) where you can experiment with different architectures, activations functions etc. I highly recommend it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example \n",
    "\n",
    "All of todays examples are going to use the keras library. Let us start with the first example we used today, the sonar dataset. Let us construct a simple neural-net for binary classification, i.e. a perceptron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=60))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xs,ys,train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3697 - binary_accuracy: 0.8759\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3698 - binary_accuracy: 0.8690\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3697 - binary_accuracy: 0.8690\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3697 - binary_accuracy: 0.8621\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3696 - binary_accuracy: 0.8621\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3695 - binary_accuracy: 0.8621\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3695 - binary_accuracy: 0.8621\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3695 - binary_accuracy: 0.8621\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3694 - binary_accuracy: 0.8621\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3694 - binary_accuracy: 0.8690\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3694 - binary_accuracy: 0.8759\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3693 - binary_accuracy: 0.8759\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3693 - binary_accuracy: 0.8690\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3693 - binary_accuracy: 0.8690\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3692 - binary_accuracy: 0.8759\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3692 - binary_accuracy: 0.8621\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3692 - binary_accuracy: 0.8552\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3691 - binary_accuracy: 0.8552\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3691 - binary_accuracy: 0.8552\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3691 - binary_accuracy: 0.8552\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3691 - binary_accuracy: 0.8690\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3690 - binary_accuracy: 0.8690\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3689 - binary_accuracy: 0.8621\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3689 - binary_accuracy: 0.8621\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3689 - binary_accuracy: 0.8621\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3688 - binary_accuracy: 0.8621\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3688 - binary_accuracy: 0.8621\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3688 - binary_accuracy: 0.8621\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3688 - binary_accuracy: 0.8621\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3687 - binary_accuracy: 0.8690\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3687 - binary_accuracy: 0.8621\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3687 - binary_accuracy: 0.8621\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3687 - binary_accuracy: 0.8621\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3686 - binary_accuracy: 0.8690\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3685 - binary_accuracy: 0.8690\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3685 - binary_accuracy: 0.8759\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3685 - binary_accuracy: 0.8759\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3685 - binary_accuracy: 0.8759\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3684 - binary_accuracy: 0.8759\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3683 - binary_accuracy: 0.8759\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3685 - binary_accuracy: 0.8690\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3683 - binary_accuracy: 0.8690\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3682 - binary_accuracy: 0.8690\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3682 - binary_accuracy: 0.8759\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3682 - binary_accuracy: 0.8759\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3682 - binary_accuracy: 0.8759\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3681 - binary_accuracy: 0.8759\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3681 - binary_accuracy: 0.8759\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3680 - binary_accuracy: 0.8759\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3680 - binary_accuracy: 0.8759\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3680 - binary_accuracy: 0.8759\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3679 - binary_accuracy: 0.8759\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3679 - binary_accuracy: 0.8759\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3679 - binary_accuracy: 0.8759\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3679 - binary_accuracy: 0.8759\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3678 - binary_accuracy: 0.8759\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3679 - binary_accuracy: 0.8759\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3678 - binary_accuracy: 0.8690\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3677 - binary_accuracy: 0.8690\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3677 - binary_accuracy: 0.8690\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3677 - binary_accuracy: 0.8690\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3676 - binary_accuracy: 0.8690\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3678 - binary_accuracy: 0.8621\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3676 - binary_accuracy: 0.8621\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3675 - binary_accuracy: 0.8690\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3675 - binary_accuracy: 0.8759\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3675 - binary_accuracy: 0.8759\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3675 - binary_accuracy: 0.8759\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3674 - binary_accuracy: 0.8759\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3674 - binary_accuracy: 0.8759\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3674 - binary_accuracy: 0.8690\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3673 - binary_accuracy: 0.8690\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3673 - binary_accuracy: 0.8759\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3672 - binary_accuracy: 0.8759\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3672 - binary_accuracy: 0.8759\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8759\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8759\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8759\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3670 - binary_accuracy: 0.8759\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3670 - binary_accuracy: 0.8759\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8759\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3671 - binary_accuracy: 0.8690\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3669 - binary_accuracy: 0.8759\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3669 - binary_accuracy: 0.8759\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3668 - binary_accuracy: 0.8759\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3668 - binary_accuracy: 0.8759\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3668 - binary_accuracy: 0.8759\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3667 - binary_accuracy: 0.8759\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3667 - binary_accuracy: 0.8759\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3667 - binary_accuracy: 0.8759\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3667 - binary_accuracy: 0.8759\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3667 - binary_accuracy: 0.8759\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3666 - binary_accuracy: 0.8759\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3665 - binary_accuracy: 0.8759\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3665 - binary_accuracy: 0.8759\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3666 - binary_accuracy: 0.8759\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.3664 - binary_accuracy: 0.8759\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3664 - binary_accuracy: 0.8759\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3664 - binary_accuracy: 0.8759\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3663 - binary_accuracy: 0.8759\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3663 - binary_accuracy: 0.8759\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3663 - binary_accuracy: 0.8759\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3662 - binary_accuracy: 0.8759\n",
      "Epoch 104/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3662 - binary_accuracy: 0.8759\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3662 - binary_accuracy: 0.8759\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3661 - binary_accuracy: 0.8759\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3661 - binary_accuracy: 0.8759\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3661 - binary_accuracy: 0.8759\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3661 - binary_accuracy: 0.8690\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3660 - binary_accuracy: 0.8690\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3660 - binary_accuracy: 0.8759\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3659 - binary_accuracy: 0.8759\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3659 - binary_accuracy: 0.8759\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3659 - binary_accuracy: 0.8759\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3659 - binary_accuracy: 0.8759\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3658 - binary_accuracy: 0.8759\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3658 - binary_accuracy: 0.8759\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3657 - binary_accuracy: 0.8759\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3657 - binary_accuracy: 0.8759\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3657 - binary_accuracy: 0.8759\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3656 - binary_accuracy: 0.8759\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3656 - binary_accuracy: 0.8759\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3656 - binary_accuracy: 0.8759\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3656 - binary_accuracy: 0.8759\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3655 - binary_accuracy: 0.8759\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3656 - binary_accuracy: 0.8759\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3654 - binary_accuracy: 0.8759\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3654 - binary_accuracy: 0.8759\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3654 - binary_accuracy: 0.8759\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3654 - binary_accuracy: 0.8759\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3653 - binary_accuracy: 0.8759\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3653 - binary_accuracy: 0.8759\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3652 - binary_accuracy: 0.8759\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3653 - binary_accuracy: 0.8759\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3652 - binary_accuracy: 0.8759\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3651 - binary_accuracy: 0.8759\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3651 - binary_accuracy: 0.8759\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3651 - binary_accuracy: 0.8759\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3650 - binary_accuracy: 0.8759\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3651 - binary_accuracy: 0.8759\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3650 - binary_accuracy: 0.8759\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3650 - binary_accuracy: 0.8759\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3649 - binary_accuracy: 0.8759\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3651 - binary_accuracy: 0.8759\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3649 - binary_accuracy: 0.8759\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3648 - binary_accuracy: 0.8759\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3648 - binary_accuracy: 0.8759\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3648 - binary_accuracy: 0.8759\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3648 - binary_accuracy: 0.8759\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3647 - binary_accuracy: 0.8759\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3647 - binary_accuracy: 0.8759\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3646 - binary_accuracy: 0.8759\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3646 - binary_accuracy: 0.8759\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3646 - binary_accuracy: 0.8759\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3645 - binary_accuracy: 0.8759\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3645 - binary_accuracy: 0.8759\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3645 - binary_accuracy: 0.8759\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3644 - binary_accuracy: 0.8759\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3644 - binary_accuracy: 0.8759\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3644 - binary_accuracy: 0.8759\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3643 - binary_accuracy: 0.8759\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3643 - binary_accuracy: 0.8759\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3643 - binary_accuracy: 0.8759\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3642 - binary_accuracy: 0.8759\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3642 - binary_accuracy: 0.8759\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3644 - binary_accuracy: 0.8759\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3642 - binary_accuracy: 0.8759\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3641 - binary_accuracy: 0.8759\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3641 - binary_accuracy: 0.8759\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3640 - binary_accuracy: 0.8759\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3640 - binary_accuracy: 0.8759\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3640 - binary_accuracy: 0.8759\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3639 - binary_accuracy: 0.8759\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3640 - binary_accuracy: 0.8759\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3639 - binary_accuracy: 0.8759\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3638 - binary_accuracy: 0.8759\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3638 - binary_accuracy: 0.8759\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3638 - binary_accuracy: 0.8759\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3638 - binary_accuracy: 0.8759\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3637 - binary_accuracy: 0.8759\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3637 - binary_accuracy: 0.8759\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3637 - binary_accuracy: 0.8759\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3636 - binary_accuracy: 0.8759\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3636 - binary_accuracy: 0.8759\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3636 - binary_accuracy: 0.8759\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3635 - binary_accuracy: 0.8759\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3635 - binary_accuracy: 0.8759\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3635 - binary_accuracy: 0.8759\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3636 - binary_accuracy: 0.8759\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3634 - binary_accuracy: 0.8759\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3634 - binary_accuracy: 0.8759\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3634 - binary_accuracy: 0.8759\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3633 - binary_accuracy: 0.8759\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3633 - binary_accuracy: 0.8759\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3632 - binary_accuracy: 0.8759\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3632 - binary_accuracy: 0.8759\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3632 - binary_accuracy: 0.8759\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3632 - binary_accuracy: 0.8759\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3631 - binary_accuracy: 0.8759\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3631 - binary_accuracy: 0.8759\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3631 - binary_accuracy: 0.8759\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3632 - binary_accuracy: 0.8759\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3630 - binary_accuracy: 0.8759\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3630 - binary_accuracy: 0.8759\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3629 - binary_accuracy: 0.8759\n",
      "Epoch 206/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3629 - binary_accuracy: 0.8759\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3630 - binary_accuracy: 0.8759\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3629 - binary_accuracy: 0.8759\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3628 - binary_accuracy: 0.8759\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3627 - binary_accuracy: 0.8759\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3627 - binary_accuracy: 0.8759\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3627 - binary_accuracy: 0.8759\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3627 - binary_accuracy: 0.8759\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3626 - binary_accuracy: 0.8759\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3626 - binary_accuracy: 0.8759\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3626 - binary_accuracy: 0.8759\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3625 - binary_accuracy: 0.8759\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3627 - binary_accuracy: 0.8759\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3627 - binary_accuracy: 0.8759\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3624 - binary_accuracy: 0.8759\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3624 - binary_accuracy: 0.8759\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3624 - binary_accuracy: 0.8759\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3624 - binary_accuracy: 0.8759\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3624 - binary_accuracy: 0.8759\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3623 - binary_accuracy: 0.8759\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3623 - binary_accuracy: 0.8759\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3622 - binary_accuracy: 0.8759\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3622 - binary_accuracy: 0.8759\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3622 - binary_accuracy: 0.8759\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3621 - binary_accuracy: 0.8759\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3621 - binary_accuracy: 0.8759\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3622 - binary_accuracy: 0.8759\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3621 - binary_accuracy: 0.8759\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3620 - binary_accuracy: 0.8759\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3620 - binary_accuracy: 0.8759\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3619 - binary_accuracy: 0.8759\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3619 - binary_accuracy: 0.8759\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3619 - binary_accuracy: 0.8759\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3619 - binary_accuracy: 0.8759\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3618 - binary_accuracy: 0.8828\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3618 - binary_accuracy: 0.8759\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3618 - binary_accuracy: 0.8759\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3617 - binary_accuracy: 0.8759\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3618 - binary_accuracy: 0.8828\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3617 - binary_accuracy: 0.8828\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3616 - binary_accuracy: 0.8759\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3618 - binary_accuracy: 0.8759\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3616 - binary_accuracy: 0.8828\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3615 - binary_accuracy: 0.8828\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3615 - binary_accuracy: 0.8828\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3616 - binary_accuracy: 0.8759\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3615 - binary_accuracy: 0.8759\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3614 - binary_accuracy: 0.8759\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3614 - binary_accuracy: 0.8759\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3614 - binary_accuracy: 0.8759\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3613 - binary_accuracy: 0.8759\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3613 - binary_accuracy: 0.8759\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3613 - binary_accuracy: 0.8759\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3613 - binary_accuracy: 0.8759\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3614 - binary_accuracy: 0.8759\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3613 - binary_accuracy: 0.8759\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3611 - binary_accuracy: 0.8759\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3611 - binary_accuracy: 0.8759\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3611 - binary_accuracy: 0.8759\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3610 - binary_accuracy: 0.8759\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3610 - binary_accuracy: 0.8759\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3610 - binary_accuracy: 0.8759\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3611 - binary_accuracy: 0.8828\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3609 - binary_accuracy: 0.8828\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3609 - binary_accuracy: 0.8828\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3609 - binary_accuracy: 0.8759\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3608 - binary_accuracy: 0.8759\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3608 - binary_accuracy: 0.8828\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3609 - binary_accuracy: 0.8759\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3607 - binary_accuracy: 0.8759\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3607 - binary_accuracy: 0.8759\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3608 - binary_accuracy: 0.8759\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3607 - binary_accuracy: 0.8828\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3606 - binary_accuracy: 0.8828\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3606 - binary_accuracy: 0.8759\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3605 - binary_accuracy: 0.8759\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3605 - binary_accuracy: 0.8828\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3606 - binary_accuracy: 0.8828\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3605 - binary_accuracy: 0.8828\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3604 - binary_accuracy: 0.8828\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3604 - binary_accuracy: 0.8828\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3604 - binary_accuracy: 0.8828\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3603 - binary_accuracy: 0.8828\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3603 - binary_accuracy: 0.8828\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3603 - binary_accuracy: 0.8828\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3603 - binary_accuracy: 0.8828\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3603 - binary_accuracy: 0.8828\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3602 - binary_accuracy: 0.8828\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3602 - binary_accuracy: 0.8828\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3601 - binary_accuracy: 0.8759\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3601 - binary_accuracy: 0.8759\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3601 - binary_accuracy: 0.8759\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3601 - binary_accuracy: 0.8759\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3600 - binary_accuracy: 0.8759\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3600 - binary_accuracy: 0.8828\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3599 - binary_accuracy: 0.8828\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3601 - binary_accuracy: 0.8828\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3599 - binary_accuracy: 0.8828\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3599 - binary_accuracy: 0.8828\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3598 - binary_accuracy: 0.8828\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3598 - binary_accuracy: 0.8828\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3598 - binary_accuracy: 0.8828\n",
      "Epoch 308/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3598 - binary_accuracy: 0.8828\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3598 - binary_accuracy: 0.8828\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3597 - binary_accuracy: 0.8828\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3596 - binary_accuracy: 0.8828\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3596 - binary_accuracy: 0.8828\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3596 - binary_accuracy: 0.8828\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3596 - binary_accuracy: 0.8828\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3595 - binary_accuracy: 0.8828\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3598 - binary_accuracy: 0.8828\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3595 - binary_accuracy: 0.8828\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3595 - binary_accuracy: 0.8828\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3594 - binary_accuracy: 0.8828\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3594 - binary_accuracy: 0.8828\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3593 - binary_accuracy: 0.8828\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3593 - binary_accuracy: 0.8828\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3593 - binary_accuracy: 0.8828\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3593 - binary_accuracy: 0.8828\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3592 - binary_accuracy: 0.8828\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3592 - binary_accuracy: 0.8828\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3592 - binary_accuracy: 0.8828\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3591 - binary_accuracy: 0.8828\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3591 - binary_accuracy: 0.8828\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3591 - binary_accuracy: 0.8828\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3590 - binary_accuracy: 0.8828\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3590 - binary_accuracy: 0.8828\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3590 - binary_accuracy: 0.8828\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3590 - binary_accuracy: 0.8828\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3589 - binary_accuracy: 0.8828\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3590 - binary_accuracy: 0.8828\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3589 - binary_accuracy: 0.8828\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3589 - binary_accuracy: 0.8828\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3589 - binary_accuracy: 0.8828\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3590 - binary_accuracy: 0.8828\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3588 - binary_accuracy: 0.8828\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3588 - binary_accuracy: 0.8828\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3587 - binary_accuracy: 0.8828\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3587 - binary_accuracy: 0.8828\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3586 - binary_accuracy: 0.8828\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3587 - binary_accuracy: 0.8828\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3586 - binary_accuracy: 0.8828\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3586 - binary_accuracy: 0.8828\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3586 - binary_accuracy: 0.8828\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3585 - binary_accuracy: 0.8828\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3586 - binary_accuracy: 0.8828\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3584 - binary_accuracy: 0.8828\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3584 - binary_accuracy: 0.8828\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3585 - binary_accuracy: 0.8828\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3584 - binary_accuracy: 0.8828\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3584 - binary_accuracy: 0.8828\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3583 - binary_accuracy: 0.8828\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3583 - binary_accuracy: 0.8828\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3583 - binary_accuracy: 0.8828\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3582 - binary_accuracy: 0.8828\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3582 - binary_accuracy: 0.8828\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3582 - binary_accuracy: 0.8828\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3582 - binary_accuracy: 0.8828\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3581 - binary_accuracy: 0.8828\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3581 - binary_accuracy: 0.8828\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3581 - binary_accuracy: 0.8828\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3580 - binary_accuracy: 0.8828\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3580 - binary_accuracy: 0.8828\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3580 - binary_accuracy: 0.8828\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3579 - binary_accuracy: 0.8828\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3579 - binary_accuracy: 0.8828\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3579 - binary_accuracy: 0.8828\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3578 - binary_accuracy: 0.8828\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3580 - binary_accuracy: 0.8828\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3578 - binary_accuracy: 0.8828\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3577 - binary_accuracy: 0.8828\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3577 - binary_accuracy: 0.8828\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3577 - binary_accuracy: 0.8828\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3577 - binary_accuracy: 0.8828\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3576 - binary_accuracy: 0.8828\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3576 - binary_accuracy: 0.8828\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3576 - binary_accuracy: 0.8828\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3575 - binary_accuracy: 0.8828\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3577 - binary_accuracy: 0.8828\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3575 - binary_accuracy: 0.8828\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3575 - binary_accuracy: 0.8828\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3575 - binary_accuracy: 0.8828\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3574 - binary_accuracy: 0.8828\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3574 - binary_accuracy: 0.8828\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3574 - binary_accuracy: 0.8828\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3573 - binary_accuracy: 0.8828\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3573 - binary_accuracy: 0.8828\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3573 - binary_accuracy: 0.8828\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3573 - binary_accuracy: 0.8828\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3572 - binary_accuracy: 0.8828\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3572 - binary_accuracy: 0.8828\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3572 - binary_accuracy: 0.8828\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3572 - binary_accuracy: 0.8828\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3571 - binary_accuracy: 0.8828\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3571 - binary_accuracy: 0.8828\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3570 - binary_accuracy: 0.8828\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3570 - binary_accuracy: 0.8828\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3570 - binary_accuracy: 0.8828\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3569 - binary_accuracy: 0.8828\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3569 - binary_accuracy: 0.8828\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3569 - binary_accuracy: 0.8828\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3569 - binary_accuracy: 0.8828\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3569 - binary_accuracy: 0.8828\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3568 - binary_accuracy: 0.8828\n",
      "Epoch 410/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3568 - binary_accuracy: 0.8828\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3567 - binary_accuracy: 0.8828\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3567 - binary_accuracy: 0.8828\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3567 - binary_accuracy: 0.8828\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3566 - binary_accuracy: 0.8828\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3567 - binary_accuracy: 0.8828\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3566 - binary_accuracy: 0.8828\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3566 - binary_accuracy: 0.8828\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3566 - binary_accuracy: 0.8828\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3566 - binary_accuracy: 0.8828\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3565 - binary_accuracy: 0.8828\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3565 - binary_accuracy: 0.8828\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3565 - binary_accuracy: 0.8828\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3564 - binary_accuracy: 0.8828\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3564 - binary_accuracy: 0.8828\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3564 - binary_accuracy: 0.8828\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3563 - binary_accuracy: 0.8828\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3563 - binary_accuracy: 0.8828\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3563 - binary_accuracy: 0.8828\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3562 - binary_accuracy: 0.8828\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3563 - binary_accuracy: 0.8828\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3563 - binary_accuracy: 0.8828\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3562 - binary_accuracy: 0.8828\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3562 - binary_accuracy: 0.8828\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3561 - binary_accuracy: 0.8828\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3562 - binary_accuracy: 0.8828\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3561 - binary_accuracy: 0.8828\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3560 - binary_accuracy: 0.8828\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3560 - binary_accuracy: 0.8828\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3559 - binary_accuracy: 0.8828\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3560 - binary_accuracy: 0.8828\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3559 - binary_accuracy: 0.8828\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3560 - binary_accuracy: 0.8828\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3558 - binary_accuracy: 0.8828\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3558 - binary_accuracy: 0.8828\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3558 - binary_accuracy: 0.8828\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3558 - binary_accuracy: 0.8828\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3558 - binary_accuracy: 0.8828\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3557 - binary_accuracy: 0.8828\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3557 - binary_accuracy: 0.8828\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3556 - binary_accuracy: 0.8828\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3556 - binary_accuracy: 0.8828\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3556 - binary_accuracy: 0.8828\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3556 - binary_accuracy: 0.8828\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3555 - binary_accuracy: 0.8828\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3555 - binary_accuracy: 0.8828\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3555 - binary_accuracy: 0.8828\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3555 - binary_accuracy: 0.8828\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3554 - binary_accuracy: 0.8828\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3554 - binary_accuracy: 0.8828\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3554 - binary_accuracy: 0.8828\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3553 - binary_accuracy: 0.8828\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3553 - binary_accuracy: 0.8828\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3553 - binary_accuracy: 0.8828\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3553 - binary_accuracy: 0.8828\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3552 - binary_accuracy: 0.8828\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3552 - binary_accuracy: 0.8828\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3552 - binary_accuracy: 0.8897\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3551 - binary_accuracy: 0.8828\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3552 - binary_accuracy: 0.8828\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3551 - binary_accuracy: 0.8828\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3552 - binary_accuracy: 0.8828\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3551 - binary_accuracy: 0.8828\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3551 - binary_accuracy: 0.8828\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3551 - binary_accuracy: 0.8828\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3550 - binary_accuracy: 0.8828\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3549 - binary_accuracy: 0.8828\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3550 - binary_accuracy: 0.8828\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3549 - binary_accuracy: 0.8828\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3548 - binary_accuracy: 0.8828\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3549 - binary_accuracy: 0.8828\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3548 - binary_accuracy: 0.8828\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3548 - binary_accuracy: 0.8828\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3547 - binary_accuracy: 0.8828\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3547 - binary_accuracy: 0.8897\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3547 - binary_accuracy: 0.8897\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3546 - binary_accuracy: 0.8897\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3546 - binary_accuracy: 0.8828\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3546 - binary_accuracy: 0.8828\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3546 - binary_accuracy: 0.8828\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3546 - binary_accuracy: 0.8828\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3545 - binary_accuracy: 0.8897\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3545 - binary_accuracy: 0.8897\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3545 - binary_accuracy: 0.8828\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3544 - binary_accuracy: 0.8828\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3544 - binary_accuracy: 0.8828\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3545 - binary_accuracy: 0.8897\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3544 - binary_accuracy: 0.8897\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3543 - binary_accuracy: 0.8897\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3543 - binary_accuracy: 0.8897\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.3544 - binary_accuracy: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ee0e4fe2f90>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24,  5],\n",
       "       [11, 23]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "tmp = np.vectorize(lambda x: helper(x,th=0.75))(y_predict.reshape(y_predict.shape[0]))\n",
    "confusion_matrix(y_test,tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another standard small example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "x_iris = iris['data']\n",
    "y_iris = iris['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "y = labeler.fit_transform(y_iris)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_iris, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 786ms/step - loss: 0.8488 - binary_accuracy: 0.4405\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7937 - binary_accuracy: 0.4405\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7452 - binary_accuracy: 0.4405\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7032 - binary_accuracy: 0.4613\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6692 - binary_accuracy: 0.5565\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6441 - binary_accuracy: 0.7738\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6244 - binary_accuracy: 0.6786\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6093 - binary_accuracy: 0.6667\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5981 - binary_accuracy: 0.6667\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5898 - binary_accuracy: 0.6667\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5835 - binary_accuracy: 0.6667\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5782 - binary_accuracy: 0.6667\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5735 - binary_accuracy: 0.6667\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5685 - binary_accuracy: 0.6667\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5632 - binary_accuracy: 0.6667\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5577 - binary_accuracy: 0.6667\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5524 - binary_accuracy: 0.6667\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5470 - binary_accuracy: 0.6667\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5409 - binary_accuracy: 0.6667\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5346 - binary_accuracy: 0.6667\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5282 - binary_accuracy: 0.6667\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5218 - binary_accuracy: 0.6667\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5155 - binary_accuracy: 0.6667\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5093 - binary_accuracy: 0.6667\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5032 - binary_accuracy: 0.6756\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4971 - binary_accuracy: 0.7262\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4911 - binary_accuracy: 0.7679\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4851 - binary_accuracy: 0.8036\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4792 - binary_accuracy: 0.8482\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4733 - binary_accuracy: 0.8571\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4674 - binary_accuracy: 0.8690\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4616 - binary_accuracy: 0.8750\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4558 - binary_accuracy: 0.8780\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4501 - binary_accuracy: 0.8810\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4446 - binary_accuracy: 0.8810\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4391 - binary_accuracy: 0.8810\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4336 - binary_accuracy: 0.8810\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4280 - binary_accuracy: 0.8810\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4223 - binary_accuracy: 0.8750\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4166 - binary_accuracy: 0.8750\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4109 - binary_accuracy: 0.8750\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4052 - binary_accuracy: 0.8750\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3996 - binary_accuracy: 0.8690\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3940 - binary_accuracy: 0.8631\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3884 - binary_accuracy: 0.8601\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3827 - binary_accuracy: 0.8601\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3767 - binary_accuracy: 0.8631\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3713 - binary_accuracy: 0.8631\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3662 - binary_accuracy: 0.8720\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3612 - binary_accuracy: 0.8750\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3565 - binary_accuracy: 0.8750\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3518 - binary_accuracy: 0.8750\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3473 - binary_accuracy: 0.8750\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3428 - binary_accuracy: 0.8750\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3385 - binary_accuracy: 0.8750\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3343 - binary_accuracy: 0.8750\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3302 - binary_accuracy: 0.8750\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3263 - binary_accuracy: 0.8750\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3225 - binary_accuracy: 0.8750\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3187 - binary_accuracy: 0.8750\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3151 - binary_accuracy: 0.8750\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3116 - binary_accuracy: 0.8750\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3080 - binary_accuracy: 0.8750\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3043 - binary_accuracy: 0.8750\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3004 - binary_accuracy: 0.8720\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2968 - binary_accuracy: 0.8720\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2938 - binary_accuracy: 0.8720\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2910 - binary_accuracy: 0.8720\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2882 - binary_accuracy: 0.8720\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2852 - binary_accuracy: 0.8720\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2823 - binary_accuracy: 0.8720\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2793 - binary_accuracy: 0.8750\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2764 - binary_accuracy: 0.8780\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2736 - binary_accuracy: 0.8780\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2707 - binary_accuracy: 0.8780\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2679 - binary_accuracy: 0.8780\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2651 - binary_accuracy: 0.8780\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2624 - binary_accuracy: 0.8780\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2598 - binary_accuracy: 0.8780\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2571 - binary_accuracy: 0.8780\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2546 - binary_accuracy: 0.8780\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2520 - binary_accuracy: 0.8780\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2494 - binary_accuracy: 0.8780\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2467 - binary_accuracy: 0.8780\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2440 - binary_accuracy: 0.8810\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2412 - binary_accuracy: 0.8810\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2385 - binary_accuracy: 0.8810\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2357 - binary_accuracy: 0.8839\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2329 - binary_accuracy: 0.8958\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2301 - binary_accuracy: 0.9077\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2273 - binary_accuracy: 0.9167\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2246 - binary_accuracy: 0.9345\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2217 - binary_accuracy: 0.9435\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2189 - binary_accuracy: 0.9464\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2161 - binary_accuracy: 0.9464\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2132 - binary_accuracy: 0.9554\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2104 - binary_accuracy: 0.9583\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2075 - binary_accuracy: 0.9613\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2046 - binary_accuracy: 0.9613\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2018 - binary_accuracy: 0.9673\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1990 - binary_accuracy: 0.9732\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1961 - binary_accuracy: 0.9762\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1933 - binary_accuracy: 0.9762\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1905 - binary_accuracy: 0.9762\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1877 - binary_accuracy: 0.9762\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1849 - binary_accuracy: 0.9762\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1821 - binary_accuracy: 0.9762\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1793 - binary_accuracy: 0.9792\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1764 - binary_accuracy: 0.9792\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1736 - binary_accuracy: 0.9851\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1707 - binary_accuracy: 0.9851\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1679 - binary_accuracy: 0.9851\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1650 - binary_accuracy: 0.9851\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1622 - binary_accuracy: 0.9851\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1594 - binary_accuracy: 0.9851\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1566 - binary_accuracy: 0.9851\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1539 - binary_accuracy: 0.9851\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1511 - binary_accuracy: 0.9851\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1484 - binary_accuracy: 0.9821\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1456 - binary_accuracy: 0.9821\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1429 - binary_accuracy: 0.9851\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1402 - binary_accuracy: 0.9851\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1375 - binary_accuracy: 0.9851\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1348 - binary_accuracy: 0.9851\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1322 - binary_accuracy: 0.9851\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1297 - binary_accuracy: 0.9851\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1273 - binary_accuracy: 0.9851\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1248 - binary_accuracy: 0.9851\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1224 - binary_accuracy: 0.9851\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1201 - binary_accuracy: 0.9851\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1177 - binary_accuracy: 0.9851\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1154 - binary_accuracy: 0.9851\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1134 - binary_accuracy: 0.9851\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1113 - binary_accuracy: 0.9851\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1091 - binary_accuracy: 0.9851\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1070 - binary_accuracy: 0.9851\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1049 - binary_accuracy: 0.9851\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1030 - binary_accuracy: 0.9851\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1011 - binary_accuracy: 0.9851\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0992 - binary_accuracy: 0.9851\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0974 - binary_accuracy: 0.9851\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0956 - binary_accuracy: 0.9851\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0939 - binary_accuracy: 0.9851\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0922 - binary_accuracy: 0.9851\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0905 - binary_accuracy: 0.9851\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0889 - binary_accuracy: 0.9851\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0874 - binary_accuracy: 0.9851\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0859 - binary_accuracy: 0.9851\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0845 - binary_accuracy: 0.9851\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0831 - binary_accuracy: 0.9851\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0818 - binary_accuracy: 0.9851\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0804 - binary_accuracy: 0.9851\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0792 - binary_accuracy: 0.9851\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0779 - binary_accuracy: 0.9851\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0767 - binary_accuracy: 0.9851\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0756 - binary_accuracy: 0.9851\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0745 - binary_accuracy: 0.9851\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0734 - binary_accuracy: 0.9851\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0723 - binary_accuracy: 0.9851\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0713 - binary_accuracy: 0.9851\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0703 - binary_accuracy: 0.9851\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0694 - binary_accuracy: 0.9851\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0685 - binary_accuracy: 0.9851\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0676 - binary_accuracy: 0.9851\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0667 - binary_accuracy: 0.9851\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0659 - binary_accuracy: 0.9851\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0651 - binary_accuracy: 0.9851\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0643 - binary_accuracy: 0.9851\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0636 - binary_accuracy: 0.9851\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0629 - binary_accuracy: 0.9851\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0622 - binary_accuracy: 0.9851\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0615 - binary_accuracy: 0.9851\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0608 - binary_accuracy: 0.9851\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0602 - binary_accuracy: 0.9851\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0596 - binary_accuracy: 0.9851\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0590 - binary_accuracy: 0.9851\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0584 - binary_accuracy: 0.9851\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0578 - binary_accuracy: 0.9851\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0573 - binary_accuracy: 0.9851\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0568 - binary_accuracy: 0.9851\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0562 - binary_accuracy: 0.9851\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0558 - binary_accuracy: 0.9851\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0553 - binary_accuracy: 0.9851\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0548 - binary_accuracy: 0.9851\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0544 - binary_accuracy: 0.9851\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0539 - binary_accuracy: 0.9851\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0535 - binary_accuracy: 0.9851\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0531 - binary_accuracy: 0.9851\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0527 - binary_accuracy: 0.9851\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0523 - binary_accuracy: 0.9851\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0520 - binary_accuracy: 0.9851\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0516 - binary_accuracy: 0.9821\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0512 - binary_accuracy: 0.9821\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0509 - binary_accuracy: 0.9821\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0506 - binary_accuracy: 0.9821\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0503 - binary_accuracy: 0.9821\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0499 - binary_accuracy: 0.9821\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0496 - binary_accuracy: 0.9821\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0493 - binary_accuracy: 0.9821\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0491 - binary_accuracy: 0.9821\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0488 - binary_accuracy: 0.9821\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0485 - binary_accuracy: 0.9821\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0482 - binary_accuracy: 0.9821\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0480 - binary_accuracy: 0.9821\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0477 - binary_accuracy: 0.9821\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0475 - binary_accuracy: 0.9821\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0473 - binary_accuracy: 0.9821\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0470 - binary_accuracy: 0.9821\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0468 - binary_accuracy: 0.9821\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0466 - binary_accuracy: 0.9821\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0464 - binary_accuracy: 0.9821\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0462 - binary_accuracy: 0.9821\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0460 - binary_accuracy: 0.9821\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0458 - binary_accuracy: 0.9821\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0456 - binary_accuracy: 0.9821\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0454 - binary_accuracy: 0.9821\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0452 - binary_accuracy: 0.9821\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0450 - binary_accuracy: 0.9851\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0448 - binary_accuracy: 0.9851\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0447 - binary_accuracy: 0.9851\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0445 - binary_accuracy: 0.9851\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0443 - binary_accuracy: 0.9851\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0442 - binary_accuracy: 0.9851\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0440 - binary_accuracy: 0.9851\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0438 - binary_accuracy: 0.9851\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0437 - binary_accuracy: 0.9851\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0435 - binary_accuracy: 0.9851\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0434 - binary_accuracy: 0.9851\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0433 - binary_accuracy: 0.9851\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0431 - binary_accuracy: 0.9851\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0430 - binary_accuracy: 0.9851\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0428 - binary_accuracy: 0.9851\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0427 - binary_accuracy: 0.9881\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0426 - binary_accuracy: 0.9881\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0424 - binary_accuracy: 0.9881\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0423 - binary_accuracy: 0.9881\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0422 - binary_accuracy: 0.9881\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0421 - binary_accuracy: 0.9881\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0420 - binary_accuracy: 0.9881\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0418 - binary_accuracy: 0.9881\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0417 - binary_accuracy: 0.9881\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0416 - binary_accuracy: 0.9881\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0415 - binary_accuracy: 0.9881\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0414 - binary_accuracy: 0.9881\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0413 - binary_accuracy: 0.9881\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0412 - binary_accuracy: 0.9881\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0411 - binary_accuracy: 0.9881\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0410 - binary_accuracy: 0.9881\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0409 - binary_accuracy: 0.9881\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0408 - binary_accuracy: 0.9881\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0407 - binary_accuracy: 0.9881\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0406 - binary_accuracy: 0.9881\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0405 - binary_accuracy: 0.9881\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0404 - binary_accuracy: 0.9881\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0403 - binary_accuracy: 0.9881\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0403 - binary_accuracy: 0.9881\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0402 - binary_accuracy: 0.9881\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0401 - binary_accuracy: 0.9881\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0400 - binary_accuracy: 0.9881\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0399 - binary_accuracy: 0.9881\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0398 - binary_accuracy: 0.9881\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0398 - binary_accuracy: 0.9881\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0397 - binary_accuracy: 0.9881\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0396 - binary_accuracy: 0.9881\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0395 - binary_accuracy: 0.9881\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0395 - binary_accuracy: 0.9881\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0394 - binary_accuracy: 0.9881\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0393 - binary_accuracy: 0.9881\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0393 - binary_accuracy: 0.9881\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0392 - binary_accuracy: 0.9881\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0391 - binary_accuracy: 0.9881\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0391 - binary_accuracy: 0.9881\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0390 - binary_accuracy: 0.9881\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0389 - binary_accuracy: 0.9881\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0389 - binary_accuracy: 0.9881\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0388 - binary_accuracy: 0.9881\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0387 - binary_accuracy: 0.9881\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0387 - binary_accuracy: 0.9881\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0386 - binary_accuracy: 0.9881\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0385 - binary_accuracy: 0.9881\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0385 - binary_accuracy: 0.9881\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0384 - binary_accuracy: 0.9881\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0384 - binary_accuracy: 0.9881\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0383 - binary_accuracy: 0.9881\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0383 - binary_accuracy: 0.9881\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0382 - binary_accuracy: 0.9881\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0381 - binary_accuracy: 0.9881\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0381 - binary_accuracy: 0.9881\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0380 - binary_accuracy: 0.9881\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0380 - binary_accuracy: 0.9881\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0379 - binary_accuracy: 0.9881\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0379 - binary_accuracy: 0.9881\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0378 - binary_accuracy: 0.9881\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0378 - binary_accuracy: 0.9881\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0377 - binary_accuracy: 0.9881\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0377 - binary_accuracy: 0.9881\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0376 - binary_accuracy: 0.9881\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0376 - binary_accuracy: 0.9881\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0375 - binary_accuracy: 0.9881\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0375 - binary_accuracy: 0.9881\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0374 - binary_accuracy: 0.9881\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0374 - binary_accuracy: 0.9881\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0374 - binary_accuracy: 0.9881\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0373 - binary_accuracy: 0.9881\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0373 - binary_accuracy: 0.9881\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0372 - binary_accuracy: 0.9881\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0372 - binary_accuracy: 0.9881\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0371 - binary_accuracy: 0.9881\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0371 - binary_accuracy: 0.9881\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0370 - binary_accuracy: 0.9881\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0370 - binary_accuracy: 0.9881\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0370 - binary_accuracy: 0.9881\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0369 - binary_accuracy: 0.9881\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0369 - binary_accuracy: 0.9881\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0368 - binary_accuracy: 0.9881\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0368 - binary_accuracy: 0.9881\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0368 - binary_accuracy: 0.9881\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0367 - binary_accuracy: 0.9881\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0367 - binary_accuracy: 0.9881\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0366 - binary_accuracy: 0.9881\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0366 - binary_accuracy: 0.9881\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0366 - binary_accuracy: 0.9881\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0365 - binary_accuracy: 0.9881\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0365 - binary_accuracy: 0.9881\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0364 - binary_accuracy: 0.9881\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0364 - binary_accuracy: 0.9881\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0364 - binary_accuracy: 0.9881\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0363 - binary_accuracy: 0.9881\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0363 - binary_accuracy: 0.9881\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0363 - binary_accuracy: 0.9881\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0362 - binary_accuracy: 0.9881\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0362 - binary_accuracy: 0.9881\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0362 - binary_accuracy: 0.9881\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0361 - binary_accuracy: 0.9881\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0361 - binary_accuracy: 0.9881\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0361 - binary_accuracy: 0.9881\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0360 - binary_accuracy: 0.9881\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0360 - binary_accuracy: 0.9881\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0360 - binary_accuracy: 0.9881\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0359 - binary_accuracy: 0.9881\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0359 - binary_accuracy: 0.9881\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0359 - binary_accuracy: 0.9881\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0358 - binary_accuracy: 0.9881\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0358 - binary_accuracy: 0.9881\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0358 - binary_accuracy: 0.9881\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0357 - binary_accuracy: 0.9881\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0357 - binary_accuracy: 0.9881\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0357 - binary_accuracy: 0.9881\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0356 - binary_accuracy: 0.9881\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0356 - binary_accuracy: 0.9881\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0356 - binary_accuracy: 0.9881\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0355 - binary_accuracy: 0.9881\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0355 - binary_accuracy: 0.9881\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0355 - binary_accuracy: 0.9881\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0354 - binary_accuracy: 0.9881\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0354 - binary_accuracy: 0.9881\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0354 - binary_accuracy: 0.9881\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0354 - binary_accuracy: 0.9881\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0353 - binary_accuracy: 0.9881\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0353 - binary_accuracy: 0.9881\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0353 - binary_accuracy: 0.9881\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0352 - binary_accuracy: 0.9881\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0352 - binary_accuracy: 0.9881\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0352 - binary_accuracy: 0.9881\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0352 - binary_accuracy: 0.9881\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0351 - binary_accuracy: 0.9881\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0351 - binary_accuracy: 0.9881\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0351 - binary_accuracy: 0.9881\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0350 - binary_accuracy: 0.9881\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0350 - binary_accuracy: 0.9881\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0350 - binary_accuracy: 0.9881\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0350 - binary_accuracy: 0.9881\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0349 - binary_accuracy: 0.9881\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0349 - binary_accuracy: 0.9881\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0349 - binary_accuracy: 0.9881\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0348 - binary_accuracy: 0.9881\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0348 - binary_accuracy: 0.9881\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0348 - binary_accuracy: 0.9881\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0348 - binary_accuracy: 0.9881\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0347 - binary_accuracy: 0.9881\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0347 - binary_accuracy: 0.9881\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0347 - binary_accuracy: 0.9881\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0347 - binary_accuracy: 0.9881\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0346 - binary_accuracy: 0.9881\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0346 - binary_accuracy: 0.9881\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0346 - binary_accuracy: 0.9881\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0346 - binary_accuracy: 0.9881\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0345 - binary_accuracy: 0.9881\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0345 - binary_accuracy: 0.9881\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0345 - binary_accuracy: 0.9881\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0345 - binary_accuracy: 0.9881\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0344 - binary_accuracy: 0.9881\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0344 - binary_accuracy: 0.9881\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0344 - binary_accuracy: 0.9881\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0344 - binary_accuracy: 0.9881\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0343 - binary_accuracy: 0.9881\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0343 - binary_accuracy: 0.9881\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0343 - binary_accuracy: 0.9881\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0343 - binary_accuracy: 0.9881\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0342 - binary_accuracy: 0.9881\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0342 - binary_accuracy: 0.9881\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0342 - binary_accuracy: 0.9881\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0342 - binary_accuracy: 0.9881\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0341 - binary_accuracy: 0.9881\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0341 - binary_accuracy: 0.9881\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0341 - binary_accuracy: 0.9881\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0341 - binary_accuracy: 0.9881\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0340 - binary_accuracy: 0.9881\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0340 - binary_accuracy: 0.9881\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0340 - binary_accuracy: 0.9881\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0340 - binary_accuracy: 0.9881\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0339 - binary_accuracy: 0.9881\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0339 - binary_accuracy: 0.9881\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0339 - binary_accuracy: 0.9881\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0339 - binary_accuracy: 0.9881\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0339 - binary_accuracy: 0.9881\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0338 - binary_accuracy: 0.9881\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0338 - binary_accuracy: 0.9881\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0338 - binary_accuracy: 0.9881\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0338 - binary_accuracy: 0.9881\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0337 - binary_accuracy: 0.9881\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0337 - binary_accuracy: 0.9881\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0337 - binary_accuracy: 0.9881\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0337 - binary_accuracy: 0.9881\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0337 - binary_accuracy: 0.9881\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0336 - binary_accuracy: 0.9881\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0336 - binary_accuracy: 0.9881\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0336 - binary_accuracy: 0.9881\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0336 - binary_accuracy: 0.9881\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0335 - binary_accuracy: 0.9881\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0335 - binary_accuracy: 0.9881\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0335 - binary_accuracy: 0.9881\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0335 - binary_accuracy: 0.9881\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0335 - binary_accuracy: 0.9881\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0334 - binary_accuracy: 0.9881\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0334 - binary_accuracy: 0.9881\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0334 - binary_accuracy: 0.9881\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0334 - binary_accuracy: 0.9881\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0334 - binary_accuracy: 0.9881\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0333 - binary_accuracy: 0.9881\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0333 - binary_accuracy: 0.9881\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0333 - binary_accuracy: 0.9881\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0333 - binary_accuracy: 0.9881\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0332 - binary_accuracy: 0.9881\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0332 - binary_accuracy: 0.9881\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0332 - binary_accuracy: 0.9881\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0332 - binary_accuracy: 0.9881\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0332 - binary_accuracy: 0.9881\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0331 - binary_accuracy: 0.9881\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0331 - binary_accuracy: 0.9881\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0331 - binary_accuracy: 0.9881\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0331 - binary_accuracy: 0.9881\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0331 - binary_accuracy: 0.9881\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0330 - binary_accuracy: 0.9881\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0330 - binary_accuracy: 0.9881\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0330 - binary_accuracy: 0.9881\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0330 - binary_accuracy: 0.9881\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0330 - binary_accuracy: 0.9881\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - binary_accuracy: 0.9881\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - binary_accuracy: 0.9881\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - binary_accuracy: 0.9881\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - binary_accuracy: 0.9881\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - binary_accuracy: 0.9881\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0328 - binary_accuracy: 0.9881\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0328 - binary_accuracy: 0.9881\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0328 - binary_accuracy: 0.9881\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0328 - binary_accuracy: 0.9881\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0328 - binary_accuracy: 0.9881\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0327 - binary_accuracy: 0.9881\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0327 - binary_accuracy: 0.9881\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0327 - binary_accuracy: 0.9881\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0327 - binary_accuracy: 0.9881\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0327 - binary_accuracy: 0.9881\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0326 - binary_accuracy: 0.9881\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0326 - binary_accuracy: 0.9881\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0326 - binary_accuracy: 0.9881\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0326 - binary_accuracy: 0.9881\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0326 - binary_accuracy: 0.9881\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0326 - binary_accuracy: 0.9881\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0325 - binary_accuracy: 0.9881\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0325 - binary_accuracy: 0.9881\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0325 - binary_accuracy: 0.9881\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0325 - binary_accuracy: 0.9881\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0325 - binary_accuracy: 0.9881\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0324 - binary_accuracy: 0.9881\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0324 - binary_accuracy: 0.9881\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0324 - binary_accuracy: 0.9881\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0324 - binary_accuracy: 0.9881\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0324 - binary_accuracy: 0.9881\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0323 - binary_accuracy: 0.9881\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0323 - binary_accuracy: 0.9881\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0323 - binary_accuracy: 0.9881\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0323 - binary_accuracy: 0.9881\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0323 - binary_accuracy: 0.9881\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0323 - binary_accuracy: 0.9881\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0322 - binary_accuracy: 0.9881\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0322 - binary_accuracy: 0.9881\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0322 - binary_accuracy: 0.9881\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0322 - binary_accuracy: 0.9881\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0322 - binary_accuracy: 0.9881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ee0e5192f10>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_dim=4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu', input_dim=4))\n",
    "model.add(Dense(3,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=500,batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11,  0,  0],\n",
       "       [ 0, 12,  0],\n",
       "       [ 0,  0, 15]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(accuracy_score(yy_test,yy_pred))\n",
    "confusion_matrix(yy_test,yy_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99762416e-01, 2.83781352e-04, 3.91601063e-09],\n",
       "       [4.03040240e-10, 2.94226102e-05, 9.99932408e-01],\n",
       "       [5.82787862e-08, 2.36688349e-02, 9.72347796e-01],\n",
       "       [1.23473670e-04, 9.99738336e-01, 1.29526918e-04],\n",
       "       [9.99774218e-01, 3.04278656e-04, 3.63716190e-09],\n",
       "       [4.59921066e-05, 9.99881923e-01, 5.33368948e-05],\n",
       "       [9.99703705e-01, 3.59422993e-04, 1.00003223e-08],\n",
       "       [9.99932766e-01, 7.54527064e-05, 3.70782155e-10],\n",
       "       [1.23216785e-04, 9.99785841e-01, 8.51499426e-05],\n",
       "       [4.42074555e-09, 5.05732431e-04, 9.99120057e-01],\n",
       "       [1.36936871e-08, 2.80270143e-03, 9.95666564e-01],\n",
       "       [2.28423680e-09, 5.28558739e-05, 9.99878287e-01],\n",
       "       [7.02421445e-08, 1.31299079e-03, 9.97837007e-01],\n",
       "       [9.99896884e-01, 1.33046007e-04, 1.98158240e-10],\n",
       "       [9.99899089e-01, 1.19986493e-04, 7.06128267e-10],\n",
       "       [7.02421445e-08, 1.31299079e-03, 9.97837007e-01],\n",
       "       [9.99900103e-01, 1.00011064e-04, 1.30487199e-09],\n",
       "       [9.98545766e-01, 1.69911142e-03, 5.96920131e-08],\n",
       "       [4.37860406e-04, 9.99435663e-01, 2.28434801e-04],\n",
       "       [9.99770761e-01, 2.77062732e-04, 6.49150289e-09],\n",
       "       [9.99617219e-01, 4.87039128e-04, 6.39462927e-09],\n",
       "       [1.93850810e-05, 9.93397534e-01, 5.31190075e-03],\n",
       "       [9.28817258e-07, 4.38048095e-01, 5.73296964e-01],\n",
       "       [3.12602396e-07, 3.43489908e-02, 9.62636650e-01],\n",
       "       [6.25098778e-07, 3.49861264e-01, 6.70691073e-01],\n",
       "       [1.65785945e-08, 3.50678383e-05, 9.99909163e-01],\n",
       "       [6.63054536e-08, 3.17430049e-01, 7.34027624e-01],\n",
       "       [1.02692911e-06, 5.88369191e-01, 4.53878224e-01],\n",
       "       [2.51262973e-04, 9.99585092e-01, 1.94853084e-04],\n",
       "       [4.94886869e-08, 1.46587025e-02, 9.81172085e-01],\n",
       "       [3.64358973e-04, 9.99632239e-01, 9.21035244e-05],\n",
       "       [2.39757937e-04, 9.99701440e-01, 8.32895967e-05],\n",
       "       [9.99382079e-01, 1.21903769e-03, 1.05810916e-09],\n",
       "       [1.97172554e-08, 8.86637950e-04, 9.98397350e-01],\n",
       "       [3.39449944e-05, 9.92569804e-01, 5.75493509e-03],\n",
       "       [7.53349383e-09, 9.21692481e-05, 9.99800265e-01],\n",
       "       [1.75739726e-04, 9.99505341e-01, 2.79164640e-04],\n",
       "       [2.52856098e-05, 9.94155884e-01, 4.26916871e-03]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Large Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "x_digits = digits['data']\n",
    "y_digits = digits['target']\n",
    "\n",
    "labeler = LabelBinarizer()\n",
    "\n",
    "yy_digits = labeler.fit_transform(y_digits)\n",
    "xx_digits = x_digits.reshape(1797,8,8,1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(xx_digits, yy_digits)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 2), padding=\"same\", activation='relu', input_shape=(8,8,1)))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 1s 2ms/step - loss: 0.3465 - binary_accuracy: 0.9033\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1256 - binary_accuracy: 0.9722\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0681 - binary_accuracy: 0.9876\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0456 - binary_accuracy: 0.9914\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0349 - binary_accuracy: 0.9947\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0280 - binary_accuracy: 0.9956\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0249 - binary_accuracy: 0.9963\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0211 - binary_accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0192 - binary_accuracy: 0.9973\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0165 - binary_accuracy: 0.9981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ee0e40cd450>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9822222222222222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[43,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 45,  0,  0,  0,  0,  2,  0,  0,  1],\n",
       "       [ 0,  0, 54,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 44,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 42,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 44,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 46,  0,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 45,  1,  0],\n",
       "       [ 0,  0,  0,  0,  0,  1,  0,  0, 37,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1, 42]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(accuracy_score(yy_test,yy_pred))\n",
    "confusion_matrix(yy_test,yy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yet Another Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(data_home='/home/kaygun/local/data/scikit_learn_data/')\n",
    "binarizer = LabelBinarizer()\n",
    "\n",
    "y = binarizer.fit_transform(faces.target.flatten()).reshape(-1,40)\n",
    "X = faces.data.flatten().reshape(-1,4096)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 1s 51ms/step - loss: 3.8686 - val_loss: 3.9931\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.7588 - val_loss: 3.7702\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.7038 - val_loss: 3.7623\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6871 - val_loss: 3.8071\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.6535 - val_loss: 3.7191\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6272 - val_loss: 3.6996\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6021 - val_loss: 3.7094\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5718 - val_loss: 3.6937\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5349 - val_loss: 3.6892\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.5027 - val_loss: 3.6459\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.4579 - val_loss: 3.6426\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 3.3962 - val_loss: 3.6206\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.3454 - val_loss: 3.5813\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.2600 - val_loss: 3.5179\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.1640 - val_loss: 3.5372\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.1332 - val_loss: 3.4186\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.0016 - val_loss: 3.3082\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8898 - val_loss: 3.2803\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.7730 - val_loss: 3.1303\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.6506 - val_loss: 2.9911\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.5399 - val_loss: 2.9403\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.4390 - val_loss: 2.7597\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.2988 - val_loss: 2.6997\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.1805 - val_loss: 2.6128\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.0792 - val_loss: 2.4789\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0062 - val_loss: 2.2909\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8838 - val_loss: 2.2553\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.8085 - val_loss: 2.1881\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7475 - val_loss: 2.1395\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.6117 - val_loss: 2.1643\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.5494 - val_loss: 1.9510\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.4459 - val_loss: 1.8998\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4060 - val_loss: 1.8516\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3503 - val_loss: 2.0116\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2834 - val_loss: 1.8292\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2530 - val_loss: 1.7001\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.1515 - val_loss: 1.6625\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.1129 - val_loss: 1.6889\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0953 - val_loss: 1.8110\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0303 - val_loss: 1.5412\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9973 - val_loss: 1.7695\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.0160 - val_loss: 1.6672\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.9828 - val_loss: 1.4964\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.8714 - val_loss: 1.5047\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8388 - val_loss: 1.4394\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.8503 - val_loss: 1.4861\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.8145 - val_loss: 1.5658\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8282 - val_loss: 1.6092\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7966 - val_loss: 1.4171\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7850 - val_loss: 1.2508\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6607 - val_loss: 1.3666\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6501 - val_loss: 1.2344\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6132 - val_loss: 1.3119\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6004 - val_loss: 1.2561\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6157 - val_loss: 1.4279\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6280 - val_loss: 1.4135\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5749 - val_loss: 1.3202\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5650 - val_loss: 1.2255\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5230 - val_loss: 1.2625\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5082 - val_loss: 1.2217\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5016 - val_loss: 1.3102\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4875 - val_loss: 1.1842\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4454 - val_loss: 1.0822\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4325 - val_loss: 1.0264\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4053 - val_loss: 1.0771\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3736 - val_loss: 1.0598\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.3721 - val_loss: 1.1109\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.3696 - val_loss: 1.0987\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.3413 - val_loss: 1.0452\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2977 - val_loss: 1.0241\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2915 - val_loss: 1.0317\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2873 - val_loss: 1.0424\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2622 - val_loss: 0.9391\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2523 - val_loss: 1.0119\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.2534 - val_loss: 1.0615\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.2564 - val_loss: 0.9094\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2276 - val_loss: 0.9923\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2266 - val_loss: 0.9685\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2307 - val_loss: 1.0606\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2346 - val_loss: 1.1426\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2207 - val_loss: 0.9970\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2292 - val_loss: 1.0227\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2080 - val_loss: 1.0679\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.2098 - val_loss: 0.9594\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.2041 - val_loss: 0.9507\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1912 - val_loss: 1.0402\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1742 - val_loss: 0.9990\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1781 - val_loss: 0.9449\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.1731 - val_loss: 1.0401\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.1370 - val_loss: 0.8981\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.1388 - val_loss: 0.9072\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1505 - val_loss: 0.9500\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1391 - val_loss: 0.9117\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1377 - val_loss: 0.9667\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1313 - val_loss: 1.0244\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1371 - val_loss: 0.9809\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1202 - val_loss: 0.9452\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1205 - val_loss: 0.9439\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1179 - val_loss: 0.9235\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1046 - val_loss: 0.9921\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1156 - val_loss: 0.9735\n",
      "Epoch 102/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1191 - val_loss: 0.9105\n",
      "Epoch 103/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1110 - val_loss: 0.9424\n",
      "Epoch 104/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0959 - val_loss: 0.8752\n",
      "Epoch 105/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1055 - val_loss: 1.1593\n",
      "Epoch 106/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1096 - val_loss: 0.8899\n",
      "Epoch 107/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0938 - val_loss: 0.9601\n",
      "Epoch 108/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0787 - val_loss: 0.9167\n",
      "Epoch 109/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0785 - val_loss: 0.9331\n",
      "Epoch 110/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0879 - val_loss: 0.8927\n",
      "Epoch 111/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0762 - val_loss: 0.9583\n",
      "Epoch 112/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0741 - val_loss: 0.8862\n",
      "Epoch 113/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0744 - val_loss: 0.9261\n",
      "Epoch 114/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0764 - val_loss: 0.9468\n",
      "Epoch 115/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0644 - val_loss: 0.8738\n",
      "Epoch 116/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0601 - val_loss: 0.9141\n",
      "Epoch 117/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0579 - val_loss: 0.8773\n",
      "Epoch 118/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0549 - val_loss: 0.9218\n",
      "Epoch 119/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0544 - val_loss: 0.8851\n",
      "Epoch 120/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0546 - val_loss: 0.8988\n",
      "Epoch 121/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0541 - val_loss: 0.9201\n",
      "Epoch 122/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0526 - val_loss: 0.8963\n",
      "Epoch 123/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0557 - val_loss: 0.9444\n",
      "Epoch 124/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0500 - val_loss: 0.8737\n",
      "Epoch 125/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0486 - val_loss: 0.9220\n",
      "Epoch 126/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0439 - val_loss: 0.9030\n",
      "Epoch 127/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0438 - val_loss: 0.9142\n",
      "Epoch 128/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0419 - val_loss: 0.9139\n",
      "Epoch 129/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0403 - val_loss: 0.9202\n",
      "Epoch 130/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0404 - val_loss: 0.9147\n",
      "Epoch 131/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0394 - val_loss: 0.8942\n",
      "Epoch 132/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0382 - val_loss: 0.9065\n",
      "Epoch 133/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0395 - val_loss: 0.9210\n",
      "Epoch 134/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0392 - val_loss: 0.9149\n",
      "Epoch 135/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0366 - val_loss: 0.9362\n",
      "Epoch 136/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0362 - val_loss: 0.8974\n",
      "Epoch 137/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0350 - val_loss: 0.9143\n",
      "Epoch 138/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0331 - val_loss: 0.9272\n",
      "Epoch 139/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0331 - val_loss: 0.9131\n",
      "Epoch 140/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0326 - val_loss: 0.9105\n",
      "Epoch 141/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0318 - val_loss: 0.9491\n",
      "Epoch 142/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0322 - val_loss: 0.9139\n",
      "Epoch 143/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0296 - val_loss: 0.9208\n",
      "Epoch 144/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0296 - val_loss: 0.9160\n",
      "Epoch 145/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0288 - val_loss: 0.9358\n",
      "Epoch 146/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 0.9076\n",
      "Epoch 147/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0275 - val_loss: 0.9252\n",
      "Epoch 148/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0267 - val_loss: 0.9234\n",
      "Epoch 149/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0270 - val_loss: 0.9282\n",
      "Epoch 150/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0257 - val_loss: 0.9319\n",
      "Epoch 151/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0246 - val_loss: 0.9245\n",
      "Epoch 152/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0260 - val_loss: 0.9354\n",
      "Epoch 153/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0254 - val_loss: 0.9195\n",
      "Epoch 154/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0241 - val_loss: 0.9265\n",
      "Epoch 155/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0239 - val_loss: 0.9310\n",
      "Epoch 156/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0241 - val_loss: 0.9435\n",
      "Epoch 157/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0243 - val_loss: 0.9176\n",
      "Epoch 158/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0234 - val_loss: 0.9210\n",
      "Epoch 159/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0223 - val_loss: 0.9435\n",
      "Epoch 160/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0214 - val_loss: 0.9208\n",
      "Epoch 161/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0211 - val_loss: 0.9359\n",
      "Epoch 162/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0212 - val_loss: 0.9237\n",
      "Epoch 163/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0204 - val_loss: 0.9315\n",
      "Epoch 164/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0197 - val_loss: 0.9332\n",
      "Epoch 165/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0194 - val_loss: 0.9518\n",
      "Epoch 166/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0192 - val_loss: 0.9167\n",
      "Epoch 167/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.9377\n",
      "Epoch 168/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0190 - val_loss: 0.9396\n",
      "Epoch 169/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0189 - val_loss: 0.9353\n",
      "Epoch 170/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0189 - val_loss: 0.9604\n",
      "Epoch 171/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0188 - val_loss: 0.9394\n",
      "Epoch 172/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0196 - val_loss: 0.9601\n",
      "Epoch 173/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0201 - val_loss: 0.9386\n",
      "Epoch 174/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0177 - val_loss: 0.9787\n",
      "Epoch 175/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0180 - val_loss: 0.9074\n",
      "Epoch 176/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0175 - val_loss: 0.9765\n",
      "Epoch 177/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0164 - val_loss: 0.9126\n",
      "Epoch 178/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0162 - val_loss: 0.9741\n",
      "Epoch 179/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0158 - val_loss: 0.9529\n",
      "Epoch 180/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0151 - val_loss: 0.9578\n",
      "Epoch 181/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0160 - val_loss: 0.9481\n",
      "Epoch 182/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0167 - val_loss: 0.9605\n",
      "Epoch 183/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0165 - val_loss: 0.9366\n",
      "Epoch 184/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0166 - val_loss: 0.9784\n",
      "Epoch 185/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0159 - val_loss: 0.9437\n",
      "Epoch 186/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0153 - val_loss: 0.9403\n",
      "Epoch 187/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0144 - val_loss: 0.9855\n",
      "Epoch 188/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0137 - val_loss: 0.9133\n",
      "Epoch 189/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 1.0034\n",
      "Epoch 190/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0136 - val_loss: 0.9323\n",
      "Epoch 191/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0130 - val_loss: 0.9648\n",
      "Epoch 192/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0126 - val_loss: 0.9725\n",
      "Epoch 193/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0124 - val_loss: 0.9465\n",
      "Epoch 194/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0121 - val_loss: 0.9642\n",
      "Epoch 195/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0120 - val_loss: 0.9716\n",
      "Epoch 196/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0118 - val_loss: 0.9629\n",
      "Epoch 197/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0116 - val_loss: 0.9717\n",
      "Epoch 198/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0115 - val_loss: 0.9655\n",
      "Epoch 199/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0113 - val_loss: 0.9585\n",
      "Epoch 200/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0112 - val_loss: 0.9719\n",
      "Epoch 201/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0110 - val_loss: 0.9620\n",
      "Epoch 202/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0108 - val_loss: 0.9758\n",
      "Epoch 203/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0106 - val_loss: 0.9635\n",
      "Epoch 204/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0107 - val_loss: 0.9725\n",
      "Epoch 205/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0105 - val_loss: 0.9742\n",
      "Epoch 206/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0102 - val_loss: 0.9654\n",
      "Epoch 207/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0102 - val_loss: 0.9724\n",
      "Epoch 208/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0101 - val_loss: 0.9657\n",
      "Epoch 209/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0100 - val_loss: 0.9743\n",
      "Epoch 210/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0100 - val_loss: 0.9621\n",
      "Epoch 211/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0099 - val_loss: 0.9767\n",
      "Epoch 212/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0096 - val_loss: 0.9641\n",
      "Epoch 213/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0096 - val_loss: 0.9703\n",
      "Epoch 214/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0094 - val_loss: 0.9798\n",
      "Epoch 215/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0093 - val_loss: 0.9690\n",
      "Epoch 216/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0091 - val_loss: 0.9795\n",
      "Epoch 217/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0090 - val_loss: 0.9731\n",
      "Epoch 218/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0090 - val_loss: 0.9703\n",
      "Epoch 219/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0088 - val_loss: 0.9809\n",
      "Epoch 220/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0087 - val_loss: 0.9933\n",
      "Epoch 221/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0086 - val_loss: 0.9875\n",
      "Epoch 222/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0085 - val_loss: 0.9775\n",
      "Epoch 223/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0085 - val_loss: 0.9808\n",
      "Epoch 224/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0082 - val_loss: 0.9922\n",
      "Epoch 225/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0083 - val_loss: 0.9690\n",
      "Epoch 226/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0081 - val_loss: 0.9913\n",
      "Epoch 227/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0080 - val_loss: 0.9843\n",
      "Epoch 228/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0079 - val_loss: 0.9800\n",
      "Epoch 229/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0079 - val_loss: 0.9973\n",
      "Epoch 230/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0078 - val_loss: 0.9800\n",
      "Epoch 231/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0076 - val_loss: 0.9918\n",
      "Epoch 232/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0077 - val_loss: 0.9803\n",
      "Epoch 233/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0075 - val_loss: 0.9940\n",
      "Epoch 234/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0074 - val_loss: 0.9815\n",
      "Epoch 235/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0074 - val_loss: 1.0067\n",
      "Epoch 236/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0074 - val_loss: 0.9808\n",
      "Epoch 237/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0072 - val_loss: 1.0171\n",
      "Epoch 238/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0071 - val_loss: 0.9819\n",
      "Epoch 239/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0071 - val_loss: 0.9787\n",
      "Epoch 240/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0071 - val_loss: 1.0103\n",
      "Epoch 241/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0069 - val_loss: 0.9918\n",
      "Epoch 242/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0069 - val_loss: 0.9900\n",
      "Epoch 243/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0068 - val_loss: 1.0086\n",
      "Epoch 244/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0068 - val_loss: 0.9926\n",
      "Epoch 245/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.9965\n",
      "Epoch 246/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0066 - val_loss: 0.9913\n",
      "Epoch 247/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 1.0046\n",
      "Epoch 248/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0064 - val_loss: 0.9896\n",
      "Epoch 249/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 1.0069\n",
      "Epoch 250/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0063 - val_loss: 0.9995\n",
      "Epoch 251/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 1.0000\n",
      "Epoch 252/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0060 - val_loss: 1.0077\n",
      "Epoch 253/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0060 - val_loss: 1.0051\n",
      "Epoch 254/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 0.9969\n",
      "Epoch 255/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0059 - val_loss: 1.0085\n",
      "Epoch 256/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0058 - val_loss: 1.0082\n",
      "Epoch 257/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0058 - val_loss: 1.0086\n",
      "Epoch 258/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 1.0020\n",
      "Epoch 259/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 1.0030\n",
      "Epoch 260/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0057 - val_loss: 1.0032\n",
      "Epoch 261/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 1.0206\n",
      "Epoch 262/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 1.0068\n",
      "Epoch 263/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 1.0110\n",
      "Epoch 264/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 1.0020\n",
      "Epoch 265/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0054 - val_loss: 1.0155\n",
      "Epoch 266/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 0.9966\n",
      "Epoch 267/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 1.0215\n",
      "Epoch 268/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 1.0237\n",
      "Epoch 269/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 0.9946\n",
      "Epoch 270/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0050 - val_loss: 1.0198\n",
      "Epoch 271/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0051 - val_loss: 1.0170\n",
      "Epoch 272/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0050 - val_loss: 1.0097\n",
      "Epoch 273/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 1.0117\n",
      "Epoch 274/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 1.0278\n",
      "Epoch 275/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0049 - val_loss: 1.0117\n",
      "Epoch 276/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0048 - val_loss: 1.0116\n",
      "Epoch 277/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 1.0126\n",
      "Epoch 278/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0047 - val_loss: 1.0223\n",
      "Epoch 279/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0048 - val_loss: 1.0084\n",
      "Epoch 280/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 1.0146\n",
      "Epoch 281/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 1.0315\n",
      "Epoch 282/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 1.0298\n",
      "Epoch 283/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0046 - val_loss: 1.0002\n",
      "Epoch 284/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0045 - val_loss: 1.0310\n",
      "Epoch 285/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0044 - val_loss: 1.0266\n",
      "Epoch 286/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0044 - val_loss: 1.0167\n",
      "Epoch 287/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 1.0189\n",
      "Epoch 288/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 1.0233\n",
      "Epoch 289/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0043 - val_loss: 1.0382\n",
      "Epoch 290/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 1.0052\n",
      "Epoch 291/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 1.0355\n",
      "Epoch 292/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 1.0228\n",
      "Epoch 293/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0042 - val_loss: 1.0310\n",
      "Epoch 294/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 1.0274\n",
      "Epoch 295/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0041 - val_loss: 1.0252\n",
      "Epoch 296/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 1.0308\n",
      "Epoch 297/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0040 - val_loss: 1.0256\n",
      "Epoch 298/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 1.0343\n",
      "Epoch 299/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0039 - val_loss: 1.0227\n",
      "Epoch 300/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0039 - val_loss: 1.0281\n",
      "Epoch 301/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 1.0371\n",
      "Epoch 302/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0038 - val_loss: 1.0224\n",
      "Epoch 303/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 1.0246\n",
      "Epoch 304/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0038 - val_loss: 1.0403\n",
      "Epoch 305/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 1.0346\n",
      "Epoch 306/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0036 - val_loss: 1.0302\n",
      "Epoch 307/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0037 - val_loss: 1.0262\n",
      "Epoch 308/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 1.0375\n",
      "Epoch 309/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0036 - val_loss: 1.0348\n",
      "Epoch 310/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 1.0301\n",
      "Epoch 311/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 1.0364\n",
      "Epoch 312/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 1.0347\n",
      "Epoch 313/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0035 - val_loss: 1.0328\n",
      "Epoch 314/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 1.0364\n",
      "Epoch 315/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 1.0381\n",
      "Epoch 316/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0034 - val_loss: 1.0300\n",
      "Epoch 317/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 1.0386\n",
      "Epoch 318/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 1.0387\n",
      "Epoch 319/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 1.0330\n",
      "Epoch 320/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0033 - val_loss: 1.0431\n",
      "Epoch 321/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 1.0403\n",
      "Epoch 322/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 1.0424\n",
      "Epoch 323/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 1.0421\n",
      "Epoch 324/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0032 - val_loss: 1.0351\n",
      "Epoch 325/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 1.0519\n",
      "Epoch 326/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0031 - val_loss: 1.0306\n",
      "Epoch 327/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0031 - val_loss: 1.0402\n",
      "Epoch 328/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 1.0458\n",
      "Epoch 329/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 1.0497\n",
      "Epoch 330/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 1.0346\n",
      "Epoch 331/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0030 - val_loss: 1.0428\n",
      "Epoch 332/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 1.0497\n",
      "Epoch 333/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0030 - val_loss: 1.0426\n",
      "Epoch 334/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.0416\n",
      "Epoch 335/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.0380\n",
      "Epoch 336/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0029 - val_loss: 1.0429\n",
      "Epoch 337/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.0527\n",
      "Epoch 338/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0029 - val_loss: 1.0430\n",
      "Epoch 339/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0028 - val_loss: 1.0537\n",
      "Epoch 340/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 1.0453\n",
      "Epoch 341/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0028 - val_loss: 1.0525\n",
      "Epoch 342/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0028 - val_loss: 1.0585\n",
      "Epoch 343/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 1.0363\n",
      "Epoch 344/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 1.0465\n",
      "Epoch 345/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 1.0652\n",
      "Epoch 346/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0027 - val_loss: 1.0438\n",
      "Epoch 347/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0027 - val_loss: 1.0660\n",
      "Epoch 348/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0026 - val_loss: 1.0509\n",
      "Epoch 349/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 1.0437\n",
      "Epoch 350/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 1.0655\n",
      "Epoch 351/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0026 - val_loss: 1.0506\n",
      "Epoch 352/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0026 - val_loss: 1.0521\n",
      "Epoch 353/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - val_loss: 1.0643\n",
      "Epoch 354/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.0568\n",
      "Epoch 355/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.0531\n",
      "Epoch 356/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0025 - val_loss: 1.0606\n",
      "Epoch 357/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.0551\n",
      "Epoch 358/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 1.0628\n",
      "Epoch 359/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.0583\n",
      "Epoch 360/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0025 - val_loss: 1.0609\n",
      "Epoch 361/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0024 - val_loss: 1.0696\n",
      "Epoch 362/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0024 - val_loss: 1.0526\n",
      "Epoch 363/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0023 - val_loss: 1.0565\n",
      "Epoch 364/1000\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.0024 - val_loss: 1.0725\n",
      "Epoch 365/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0023 - val_loss: 1.0631\n",
      "Epoch 366/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 1.0483\n",
      "Epoch 367/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 1.0680\n",
      "Epoch 368/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0023 - val_loss: 1.0747\n",
      "Epoch 369/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 1.0578\n",
      "Epoch 370/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0023 - val_loss: 1.0616\n",
      "Epoch 371/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - val_loss: 1.0620\n",
      "Epoch 372/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0022 - val_loss: 1.0622\n",
      "Epoch 373/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 1.0598\n",
      "Epoch 374/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0022 - val_loss: 1.0612\n",
      "Epoch 375/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0022 - val_loss: 1.0645\n",
      "Epoch 376/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - val_loss: 1.0627\n",
      "Epoch 377/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 1.0652\n",
      "Epoch 378/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0021 - val_loss: 1.0681\n",
      "Epoch 379/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 1.0701\n",
      "Epoch 380/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0021 - val_loss: 1.0638\n",
      "Epoch 381/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 1.0649\n",
      "Epoch 382/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0021 - val_loss: 1.0756\n",
      "Epoch 383/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0020 - val_loss: 1.0703\n",
      "Epoch 384/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0020 - val_loss: 1.0697\n",
      "Epoch 385/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0020 - val_loss: 1.0732\n",
      "Epoch 386/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0020 - val_loss: 1.0758\n",
      "Epoch 387/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.0705\n",
      "Epoch 388/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.0751\n",
      "Epoch 389/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0020 - val_loss: 1.0796\n",
      "Epoch 390/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.0735\n",
      "Epoch 391/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0020 - val_loss: 1.0747\n",
      "Epoch 392/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.0784\n",
      "Epoch 393/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.0738\n",
      "Epoch 394/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 1.0815\n",
      "Epoch 395/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0019 - val_loss: 1.0718\n",
      "Epoch 396/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.0712\n",
      "Epoch 397/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 1.0813\n",
      "Epoch 398/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 1.0794\n",
      "Epoch 399/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0019 - val_loss: 1.0655\n",
      "Epoch 400/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0019 - val_loss: 1.0788\n",
      "Epoch 401/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 1.0789\n",
      "Epoch 402/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 1.0800\n",
      "Epoch 403/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 1.0766\n",
      "Epoch 404/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 1.0876\n",
      "Epoch 405/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 1.0753\n",
      "Epoch 406/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0018 - val_loss: 1.0829\n",
      "Epoch 407/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0017 - val_loss: 1.0819\n",
      "Epoch 408/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0018 - val_loss: 1.0789\n",
      "Epoch 409/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0017 - val_loss: 1.0853\n",
      "Epoch 410/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0017 - val_loss: 1.0890\n",
      "Epoch 411/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 1.0732\n",
      "Epoch 412/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0017 - val_loss: 1.0746\n",
      "Epoch 413/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0017 - val_loss: 1.1007\n",
      "Epoch 414/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 1.0811\n",
      "Epoch 415/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 1.0766\n",
      "Epoch 416/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 1.0937\n",
      "Epoch 417/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0017 - val_loss: 1.0912\n",
      "Epoch 418/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.0763\n",
      "Epoch 419/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.0857\n",
      "Epoch 420/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0016 - val_loss: 1.0932\n",
      "Epoch 421/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 1.0834\n",
      "Epoch 422/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 1.0939\n",
      "Epoch 423/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 1.0909\n",
      "Epoch 424/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0016 - val_loss: 1.0901\n",
      "Epoch 425/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0016 - val_loss: 1.0890\n",
      "Epoch 426/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0016 - val_loss: 1.0849\n",
      "Epoch 427/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0016 - val_loss: 1.0920\n",
      "Epoch 428/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.0908\n",
      "Epoch 429/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.0921\n",
      "Epoch 430/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.0888\n",
      "Epoch 431/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0015 - val_loss: 1.0926\n",
      "Epoch 432/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.0979\n",
      "Epoch 433/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0015 - val_loss: 1.0912\n",
      "Epoch 434/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.0864\n",
      "Epoch 435/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.1034\n",
      "Epoch 436/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0015 - val_loss: 1.0938\n",
      "Epoch 437/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.0877\n",
      "Epoch 438/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0015 - val_loss: 1.0998\n",
      "Epoch 439/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.0951\n",
      "Epoch 440/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0014 - val_loss: 1.0917\n",
      "Epoch 441/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.1014\n",
      "Epoch 442/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.0849\n",
      "Epoch 443/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.1017\n",
      "Epoch 444/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.1069\n",
      "Epoch 445/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.0971\n",
      "Epoch 446/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.0960\n",
      "Epoch 447/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.1034\n",
      "Epoch 448/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.1015\n",
      "Epoch 449/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0014 - val_loss: 1.0992\n",
      "Epoch 450/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0014 - val_loss: 1.1057\n",
      "Epoch 451/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.0988\n",
      "Epoch 452/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1060\n",
      "Epoch 453/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1056\n",
      "Epoch 454/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.1029\n",
      "Epoch 455/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0013 - val_loss: 1.1012\n",
      "Epoch 456/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1003\n",
      "Epoch 457/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0013 - val_loss: 1.1074\n",
      "Epoch 458/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1091\n",
      "Epoch 459/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1038\n",
      "Epoch 460/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1018\n",
      "Epoch 461/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1074\n",
      "Epoch 462/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1093\n",
      "Epoch 463/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0013 - val_loss: 1.1003\n",
      "Epoch 464/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1117\n",
      "Epoch 465/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1094\n",
      "Epoch 466/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1006\n",
      "Epoch 467/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1119\n",
      "Epoch 468/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1123\n",
      "Epoch 469/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1113\n",
      "Epoch 470/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0012 - val_loss: 1.1090\n",
      "Epoch 471/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1104\n",
      "Epoch 472/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1058\n",
      "Epoch 473/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1054\n",
      "Epoch 474/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1129\n",
      "Epoch 475/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1149\n",
      "Epoch 476/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1117\n",
      "Epoch 477/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1036\n",
      "Epoch 478/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0012 - val_loss: 1.1092\n",
      "Epoch 479/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0012 - val_loss: 1.1204\n",
      "Epoch 480/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1052\n",
      "Epoch 481/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1045\n",
      "Epoch 482/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1213\n",
      "Epoch 483/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1123\n",
      "Epoch 484/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0011 - val_loss: 1.1110\n",
      "Epoch 485/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1137\n",
      "Epoch 486/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1229\n",
      "Epoch 487/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1151\n",
      "Epoch 488/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1091\n",
      "Epoch 489/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1222\n",
      "Epoch 490/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0011 - val_loss: 1.1170\n",
      "Epoch 491/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1151\n",
      "Epoch 492/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1201\n",
      "Epoch 493/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1251\n",
      "Epoch 494/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1187\n",
      "Epoch 495/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0011 - val_loss: 1.1194\n",
      "Epoch 496/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1277\n",
      "Epoch 497/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0010 - val_loss: 1.1234\n",
      "Epoch 498/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1213\n",
      "Epoch 499/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1106\n",
      "Epoch 500/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1309\n",
      "Epoch 501/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1289\n",
      "Epoch 502/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1188\n",
      "Epoch 503/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1222\n",
      "Epoch 504/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1227\n",
      "Epoch 505/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0010 - val_loss: 1.1210\n",
      "Epoch 506/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.9428e-04 - val_loss: 1.1261\n",
      "Epoch 507/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.8914e-04 - val_loss: 1.1262\n",
      "Epoch 508/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.8111e-04 - val_loss: 1.1234\n",
      "Epoch 509/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.7915e-04 - val_loss: 1.1272\n",
      "Epoch 510/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.7356e-04 - val_loss: 1.1275\n",
      "Epoch 511/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.6674e-04 - val_loss: 1.1282\n",
      "Epoch 512/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.6583e-04 - val_loss: 1.1262\n",
      "Epoch 513/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.5424e-04 - val_loss: 1.1321\n",
      "Epoch 514/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.4944e-04 - val_loss: 1.1241\n",
      "Epoch 515/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.4510e-04 - val_loss: 1.1225\n",
      "Epoch 516/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 9.4144e-04 - val_loss: 1.1285\n",
      "Epoch 517/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 9.3308e-04 - val_loss: 1.1257\n",
      "Epoch 518/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.4120e-04 - val_loss: 1.1274\n",
      "Epoch 519/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.2801e-04 - val_loss: 1.1296\n",
      "Epoch 520/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.2171e-04 - val_loss: 1.1357\n",
      "Epoch 521/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.1178e-04 - val_loss: 1.1267\n",
      "Epoch 522/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.1584e-04 - val_loss: 1.1299\n",
      "Epoch 523/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.0695e-04 - val_loss: 1.1387\n",
      "Epoch 524/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.0469e-04 - val_loss: 1.1249\n",
      "Epoch 525/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9.0036e-04 - val_loss: 1.1333\n",
      "Epoch 526/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.9604e-04 - val_loss: 1.1351\n",
      "Epoch 527/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.8732e-04 - val_loss: 1.1379\n",
      "Epoch 528/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.8169e-04 - val_loss: 1.1334\n",
      "Epoch 529/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.7775e-04 - val_loss: 1.1326\n",
      "Epoch 530/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.7664e-04 - val_loss: 1.1361\n",
      "Epoch 531/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.6962e-04 - val_loss: 1.1325\n",
      "Epoch 532/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.6449e-04 - val_loss: 1.1411\n",
      "Epoch 533/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.6288e-04 - val_loss: 1.1349\n",
      "Epoch 534/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.5493e-04 - val_loss: 1.1334\n",
      "Epoch 535/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8.5597e-04 - val_loss: 1.1392\n",
      "Epoch 536/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.4498e-04 - val_loss: 1.1356\n",
      "Epoch 537/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.4386e-04 - val_loss: 1.1350\n",
      "Epoch 538/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.3872e-04 - val_loss: 1.1370\n",
      "Epoch 539/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.3689e-04 - val_loss: 1.1332\n",
      "Epoch 540/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.3017e-04 - val_loss: 1.1357\n",
      "Epoch 541/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.2784e-04 - val_loss: 1.1378\n",
      "Epoch 542/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8.2140e-04 - val_loss: 1.1419\n",
      "Epoch 543/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.2227e-04 - val_loss: 1.1359\n",
      "Epoch 544/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 8.1083e-04 - val_loss: 1.1388\n",
      "Epoch 545/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.1026e-04 - val_loss: 1.1368\n",
      "Epoch 546/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 8.1070e-04 - val_loss: 1.1353\n",
      "Epoch 547/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7.9816e-04 - val_loss: 1.1382\n",
      "Epoch 548/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 8.0366e-04 - val_loss: 1.1427\n",
      "Epoch 549/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8.0085e-04 - val_loss: 1.1457\n",
      "Epoch 550/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7.9894e-04 - val_loss: 1.1389\n",
      "Epoch 551/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7.8365e-04 - val_loss: 1.1475\n",
      "Epoch 552/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.8274e-04 - val_loss: 1.1434\n",
      "Epoch 553/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.7949e-04 - val_loss: 1.1381\n",
      "Epoch 554/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.7613e-04 - val_loss: 1.1451\n",
      "Epoch 555/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7.7122e-04 - val_loss: 1.1417\n",
      "Epoch 556/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.6325e-04 - val_loss: 1.1438\n",
      "Epoch 557/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.6643e-04 - val_loss: 1.1388\n",
      "Epoch 558/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.6107e-04 - val_loss: 1.1480\n",
      "Epoch 559/1000\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 7.5858e-04 - val_loss: 1.1495\n",
      "Epoch 560/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.5270e-04 - val_loss: 1.1407\n",
      "Epoch 561/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.5144e-04 - val_loss: 1.1471\n",
      "Epoch 562/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4216e-04 - val_loss: 1.1480\n",
      "Epoch 563/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.4090e-04 - val_loss: 1.1414\n",
      "Epoch 564/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3682e-04 - val_loss: 1.1388\n",
      "Epoch 565/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.3494e-04 - val_loss: 1.1523\n",
      "Epoch 566/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.3197e-04 - val_loss: 1.1492\n",
      "Epoch 567/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.2288e-04 - val_loss: 1.1410\n",
      "Epoch 568/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7.1948e-04 - val_loss: 1.1451\n",
      "Epoch 569/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.1987e-04 - val_loss: 1.1509\n",
      "Epoch 570/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.1311e-04 - val_loss: 1.1477\n",
      "Epoch 571/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.0879e-04 - val_loss: 1.1471\n",
      "Epoch 572/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 7.0364e-04 - val_loss: 1.1489\n",
      "Epoch 573/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 7.0488e-04 - val_loss: 1.1502\n",
      "Epoch 574/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7.0279e-04 - val_loss: 1.1424\n",
      "Epoch 575/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.9489e-04 - val_loss: 1.1507\n",
      "Epoch 576/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.9130e-04 - val_loss: 1.1537\n",
      "Epoch 577/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.8902e-04 - val_loss: 1.1491\n",
      "Epoch 578/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.8827e-04 - val_loss: 1.1453\n",
      "Epoch 579/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.8328e-04 - val_loss: 1.1561\n",
      "Epoch 580/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.8150e-04 - val_loss: 1.1520\n",
      "Epoch 581/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.7714e-04 - val_loss: 1.1462\n",
      "Epoch 582/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 6.7307e-04 - val_loss: 1.1504\n",
      "Epoch 583/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.6938e-04 - val_loss: 1.1535\n",
      "Epoch 584/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.6767e-04 - val_loss: 1.1532\n",
      "Epoch 585/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.7004e-04 - val_loss: 1.1566\n",
      "Epoch 586/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.6363e-04 - val_loss: 1.1544\n",
      "Epoch 587/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.6565e-04 - val_loss: 1.1560\n",
      "Epoch 588/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.5668e-04 - val_loss: 1.1527\n",
      "Epoch 589/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.5009e-04 - val_loss: 1.1569\n",
      "Epoch 590/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.4924e-04 - val_loss: 1.1563\n",
      "Epoch 591/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.4518e-04 - val_loss: 1.1538\n",
      "Epoch 592/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.4255e-04 - val_loss: 1.1476\n",
      "Epoch 593/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.3960e-04 - val_loss: 1.1569\n",
      "Epoch 594/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.3501e-04 - val_loss: 1.1615\n",
      "Epoch 595/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.3248e-04 - val_loss: 1.1577\n",
      "Epoch 596/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.2811e-04 - val_loss: 1.1588\n",
      "Epoch 597/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.2790e-04 - val_loss: 1.1575\n",
      "Epoch 598/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.2681e-04 - val_loss: 1.1627\n",
      "Epoch 599/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 6.2062e-04 - val_loss: 1.1557\n",
      "Epoch 600/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.2334e-04 - val_loss: 1.1527\n",
      "Epoch 601/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.1213e-04 - val_loss: 1.1700\n",
      "Epoch 602/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.1524e-04 - val_loss: 1.1686\n",
      "Epoch 603/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.0991e-04 - val_loss: 1.1598\n",
      "Epoch 604/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6.0808e-04 - val_loss: 1.1548\n",
      "Epoch 605/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6.0728e-04 - val_loss: 1.1644\n",
      "Epoch 606/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9920e-04 - val_loss: 1.1605\n",
      "Epoch 607/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9740e-04 - val_loss: 1.1607\n",
      "Epoch 608/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9807e-04 - val_loss: 1.1639\n",
      "Epoch 609/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9587e-04 - val_loss: 1.1657\n",
      "Epoch 610/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.9743e-04 - val_loss: 1.1584\n",
      "Epoch 611/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.8609e-04 - val_loss: 1.1688\n",
      "Epoch 612/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.8563e-04 - val_loss: 1.1707\n",
      "Epoch 613/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.8260e-04 - val_loss: 1.1620\n",
      "Epoch 614/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.7715e-04 - val_loss: 1.1606\n",
      "Epoch 615/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.7942e-04 - val_loss: 1.1645\n",
      "Epoch 616/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.7534e-04 - val_loss: 1.1668\n",
      "Epoch 617/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.7353e-04 - val_loss: 1.1689\n",
      "Epoch 618/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.7096e-04 - val_loss: 1.1621\n",
      "Epoch 619/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.6439e-04 - val_loss: 1.1643\n",
      "Epoch 620/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.6301e-04 - val_loss: 1.1673\n",
      "Epoch 621/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.6086e-04 - val_loss: 1.1604\n",
      "Epoch 622/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.5788e-04 - val_loss: 1.1640\n",
      "Epoch 623/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.5616e-04 - val_loss: 1.1670\n",
      "Epoch 624/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.5438e-04 - val_loss: 1.1622\n",
      "Epoch 625/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.5481e-04 - val_loss: 1.1601\n",
      "Epoch 626/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.4594e-04 - val_loss: 1.1725\n",
      "Epoch 627/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.4577e-04 - val_loss: 1.1762\n",
      "Epoch 628/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.4339e-04 - val_loss: 1.1662\n",
      "Epoch 629/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.4103e-04 - val_loss: 1.1607\n",
      "Epoch 630/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.4010e-04 - val_loss: 1.1747\n",
      "Epoch 631/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.3525e-04 - val_loss: 1.1727\n",
      "Epoch 632/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.3211e-04 - val_loss: 1.1656\n",
      "Epoch 633/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.3245e-04 - val_loss: 1.1620\n",
      "Epoch 634/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5.2835e-04 - val_loss: 1.1781\n",
      "Epoch 635/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.2998e-04 - val_loss: 1.1745\n",
      "Epoch 636/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.2380e-04 - val_loss: 1.1643\n",
      "Epoch 637/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.2341e-04 - val_loss: 1.1665\n",
      "Epoch 638/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.1930e-04 - val_loss: 1.1747\n",
      "Epoch 639/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.1613e-04 - val_loss: 1.1723\n",
      "Epoch 640/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5.1464e-04 - val_loss: 1.1737\n",
      "Epoch 641/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.1232e-04 - val_loss: 1.1769\n",
      "Epoch 642/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5.1482e-04 - val_loss: 1.1729\n",
      "Epoch 643/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.0973e-04 - val_loss: 1.1658\n",
      "Epoch 644/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.0805e-04 - val_loss: 1.1790\n",
      "Epoch 645/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.0713e-04 - val_loss: 1.1743\n",
      "Epoch 646/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5.0389e-04 - val_loss: 1.1740\n",
      "Epoch 647/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9805e-04 - val_loss: 1.1755\n",
      "Epoch 648/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9557e-04 - val_loss: 1.1728\n",
      "Epoch 649/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9582e-04 - val_loss: 1.1780\n",
      "Epoch 650/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9227e-04 - val_loss: 1.1780\n",
      "Epoch 651/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9083e-04 - val_loss: 1.1683\n",
      "Epoch 652/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.9116e-04 - val_loss: 1.1824\n",
      "Epoch 653/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.8969e-04 - val_loss: 1.1782\n",
      "Epoch 654/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.8412e-04 - val_loss: 1.1837\n",
      "Epoch 655/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.8197e-04 - val_loss: 1.1857\n",
      "Epoch 656/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.7847e-04 - val_loss: 1.1776\n",
      "Epoch 657/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.7640e-04 - val_loss: 1.1807\n",
      "Epoch 658/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.7891e-04 - val_loss: 1.1791\n",
      "Epoch 659/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.7292e-04 - val_loss: 1.1771\n",
      "Epoch 660/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.7186e-04 - val_loss: 1.1780\n",
      "Epoch 661/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.7046e-04 - val_loss: 1.1805\n",
      "Epoch 662/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.6676e-04 - val_loss: 1.1825\n",
      "Epoch 663/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.6592e-04 - val_loss: 1.1759\n",
      "Epoch 664/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.6430e-04 - val_loss: 1.1873\n",
      "Epoch 665/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.6183e-04 - val_loss: 1.1860\n",
      "Epoch 666/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.6038e-04 - val_loss: 1.1772\n",
      "Epoch 667/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.5716e-04 - val_loss: 1.1834\n",
      "Epoch 668/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.5504e-04 - val_loss: 1.1879\n",
      "Epoch 669/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5263e-04 - val_loss: 1.1818\n",
      "Epoch 670/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.5551e-04 - val_loss: 1.1807\n",
      "Epoch 671/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.5050e-04 - val_loss: 1.1754\n",
      "Epoch 672/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.5045e-04 - val_loss: 1.1853\n",
      "Epoch 673/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.4840e-04 - val_loss: 1.1867\n",
      "Epoch 674/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.4438e-04 - val_loss: 1.1838\n",
      "Epoch 675/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.4050e-04 - val_loss: 1.1841\n",
      "Epoch 676/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.4001e-04 - val_loss: 1.1866\n",
      "Epoch 677/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.3953e-04 - val_loss: 1.1907\n",
      "Epoch 678/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.3501e-04 - val_loss: 1.1859\n",
      "Epoch 679/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.3481e-04 - val_loss: 1.1785\n",
      "Epoch 680/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.3209e-04 - val_loss: 1.1863\n",
      "Epoch 681/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.3216e-04 - val_loss: 1.1873\n",
      "Epoch 682/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.2830e-04 - val_loss: 1.1887\n",
      "Epoch 683/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.2945e-04 - val_loss: 1.1878\n",
      "Epoch 684/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.2377e-04 - val_loss: 1.1939\n",
      "Epoch 685/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.2526e-04 - val_loss: 1.1904\n",
      "Epoch 686/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.2093e-04 - val_loss: 1.1837\n",
      "Epoch 687/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.2058e-04 - val_loss: 1.1905\n",
      "Epoch 688/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.1736e-04 - val_loss: 1.1941\n",
      "Epoch 689/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.1632e-04 - val_loss: 1.1962\n",
      "Epoch 690/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.1300e-04 - val_loss: 1.1885\n",
      "Epoch 691/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.1237e-04 - val_loss: 1.1887\n",
      "Epoch 692/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.0966e-04 - val_loss: 1.1972\n",
      "Epoch 693/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.0950e-04 - val_loss: 1.1928\n",
      "Epoch 694/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.0649e-04 - val_loss: 1.1890\n",
      "Epoch 695/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.0536e-04 - val_loss: 1.1901\n",
      "Epoch 696/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.0235e-04 - val_loss: 1.1927\n",
      "Epoch 697/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4.0320e-04 - val_loss: 1.1946\n",
      "Epoch 698/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.9935e-04 - val_loss: 1.1943\n",
      "Epoch 699/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.9938e-04 - val_loss: 1.1940\n",
      "Epoch 700/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.9959e-04 - val_loss: 1.1958\n",
      "Epoch 701/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.9570e-04 - val_loss: 1.1907\n",
      "Epoch 702/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.9317e-04 - val_loss: 1.1987\n",
      "Epoch 703/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.9296e-04 - val_loss: 1.1963\n",
      "Epoch 704/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.8967e-04 - val_loss: 1.1910\n",
      "Epoch 705/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.8906e-04 - val_loss: 1.1915\n",
      "Epoch 706/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.8735e-04 - val_loss: 1.1979\n",
      "Epoch 707/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.8635e-04 - val_loss: 1.1981\n",
      "Epoch 708/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.8406e-04 - val_loss: 1.1927\n",
      "Epoch 709/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.8383e-04 - val_loss: 1.1935\n",
      "Epoch 710/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.8225e-04 - val_loss: 1.2020\n",
      "Epoch 711/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.7935e-04 - val_loss: 1.1970\n",
      "Epoch 712/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.7785e-04 - val_loss: 1.1937\n",
      "Epoch 713/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.7561e-04 - val_loss: 1.1995\n",
      "Epoch 714/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.7737e-04 - val_loss: 1.1950\n",
      "Epoch 715/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.7375e-04 - val_loss: 1.2059\n",
      "Epoch 716/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.7086e-04 - val_loss: 1.1996\n",
      "Epoch 717/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.7030e-04 - val_loss: 1.2008\n",
      "Epoch 718/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6912e-04 - val_loss: 1.1974\n",
      "Epoch 719/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.6756e-04 - val_loss: 1.1998\n",
      "Epoch 720/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6589e-04 - val_loss: 1.2012\n",
      "Epoch 721/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6713e-04 - val_loss: 1.2016\n",
      "Epoch 722/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.6271e-04 - val_loss: 1.2045\n",
      "Epoch 723/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.6456e-04 - val_loss: 1.2023\n",
      "Epoch 724/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.5990e-04 - val_loss: 1.2065\n",
      "Epoch 725/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.5947e-04 - val_loss: 1.2061\n",
      "Epoch 726/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.5769e-04 - val_loss: 1.2000\n",
      "Epoch 727/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.5573e-04 - val_loss: 1.1942\n",
      "Epoch 728/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.5504e-04 - val_loss: 1.2060\n",
      "Epoch 729/1000\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3.5470e-04 - val_loss: 1.2159\n",
      "Epoch 730/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.5179e-04 - val_loss: 1.2031\n",
      "Epoch 731/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.4825e-04 - val_loss: 1.2021\n",
      "Epoch 732/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.4770e-04 - val_loss: 1.2078\n",
      "Epoch 733/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.4673e-04 - val_loss: 1.2042\n",
      "Epoch 734/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.4422e-04 - val_loss: 1.2115\n",
      "Epoch 735/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.4320e-04 - val_loss: 1.2108\n",
      "Epoch 736/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.4221e-04 - val_loss: 1.2059\n",
      "Epoch 737/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.4101e-04 - val_loss: 1.2070\n",
      "Epoch 738/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.3966e-04 - val_loss: 1.2064\n",
      "Epoch 739/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.3838e-04 - val_loss: 1.2095\n",
      "Epoch 740/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.3698e-04 - val_loss: 1.2101\n",
      "Epoch 741/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.3416e-04 - val_loss: 1.2073\n",
      "Epoch 742/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.3300e-04 - val_loss: 1.2103\n",
      "Epoch 743/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3254e-04 - val_loss: 1.2110\n",
      "Epoch 744/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.3150e-04 - val_loss: 1.2126\n",
      "Epoch 745/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.3004e-04 - val_loss: 1.2092\n",
      "Epoch 746/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.2872e-04 - val_loss: 1.2091\n",
      "Epoch 747/1000\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 3.2810e-04 - val_loss: 1.2116\n",
      "Epoch 748/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.2562e-04 - val_loss: 1.2083\n",
      "Epoch 749/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.2443e-04 - val_loss: 1.2141\n",
      "Epoch 750/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.2274e-04 - val_loss: 1.2152\n",
      "Epoch 751/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.2090e-04 - val_loss: 1.2092\n",
      "Epoch 752/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.2107e-04 - val_loss: 1.2064\n",
      "Epoch 753/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.1957e-04 - val_loss: 1.2120\n",
      "Epoch 754/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.1804e-04 - val_loss: 1.2107\n",
      "Epoch 755/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.1696e-04 - val_loss: 1.2140\n",
      "Epoch 756/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.1510e-04 - val_loss: 1.2160\n",
      "Epoch 757/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.1449e-04 - val_loss: 1.2156\n",
      "Epoch 758/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.1340e-04 - val_loss: 1.2097\n",
      "Epoch 759/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.1220e-04 - val_loss: 1.2156\n",
      "Epoch 760/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.1021e-04 - val_loss: 1.2190\n",
      "Epoch 761/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.0941e-04 - val_loss: 1.2165\n",
      "Epoch 762/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.0702e-04 - val_loss: 1.2146\n",
      "Epoch 763/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.0585e-04 - val_loss: 1.2145\n",
      "Epoch 764/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.0586e-04 - val_loss: 1.2187\n",
      "Epoch 765/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.0421e-04 - val_loss: 1.2202\n",
      "Epoch 766/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.0206e-04 - val_loss: 1.2122\n",
      "Epoch 767/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.0155e-04 - val_loss: 1.2116\n",
      "Epoch 768/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.9989e-04 - val_loss: 1.2164\n",
      "Epoch 769/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9864e-04 - val_loss: 1.2201\n",
      "Epoch 770/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9765e-04 - val_loss: 1.2181\n",
      "Epoch 771/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9744e-04 - val_loss: 1.2168\n",
      "Epoch 772/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9548e-04 - val_loss: 1.2223\n",
      "Epoch 773/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9517e-04 - val_loss: 1.2192\n",
      "Epoch 774/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9362e-04 - val_loss: 1.2139\n",
      "Epoch 775/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.9476e-04 - val_loss: 1.2180\n",
      "Epoch 776/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.9227e-04 - val_loss: 1.2253\n",
      "Epoch 777/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8974e-04 - val_loss: 1.2173\n",
      "Epoch 778/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8979e-04 - val_loss: 1.2166\n",
      "Epoch 779/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8823e-04 - val_loss: 1.2191\n",
      "Epoch 780/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8633e-04 - val_loss: 1.2245\n",
      "Epoch 781/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.8587e-04 - val_loss: 1.2246\n",
      "Epoch 782/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.8483e-04 - val_loss: 1.2156\n",
      "Epoch 783/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.8443e-04 - val_loss: 1.2237\n",
      "Epoch 784/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.8257e-04 - val_loss: 1.2236\n",
      "Epoch 785/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.8210e-04 - val_loss: 1.2214\n",
      "Epoch 786/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.8095e-04 - val_loss: 1.2236\n",
      "Epoch 787/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.7975e-04 - val_loss: 1.2335\n",
      "Epoch 788/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.7948e-04 - val_loss: 1.2236\n",
      "Epoch 789/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.7948e-04 - val_loss: 1.2218\n",
      "Epoch 790/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.7625e-04 - val_loss: 1.2313\n",
      "Epoch 791/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.7496e-04 - val_loss: 1.2290\n",
      "Epoch 792/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.7399e-04 - val_loss: 1.2204\n",
      "Epoch 793/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.7204e-04 - val_loss: 1.2192\n",
      "Epoch 794/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.7187e-04 - val_loss: 1.2251\n",
      "Epoch 795/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.7089e-04 - val_loss: 1.2329\n",
      "Epoch 796/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6953e-04 - val_loss: 1.2303\n",
      "Epoch 797/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.6976e-04 - val_loss: 1.2221\n",
      "Epoch 798/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6789e-04 - val_loss: 1.2235\n",
      "Epoch 799/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6639e-04 - val_loss: 1.2222\n",
      "Epoch 800/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.6561e-04 - val_loss: 1.2271\n",
      "Epoch 801/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6433e-04 - val_loss: 1.2291\n",
      "Epoch 802/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6363e-04 - val_loss: 1.2233\n",
      "Epoch 803/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6196e-04 - val_loss: 1.2277\n",
      "Epoch 804/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6146e-04 - val_loss: 1.2314\n",
      "Epoch 805/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5964e-04 - val_loss: 1.2288\n",
      "Epoch 806/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5810e-04 - val_loss: 1.2261\n",
      "Epoch 807/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5805e-04 - val_loss: 1.2274\n",
      "Epoch 808/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5675e-04 - val_loss: 1.2321\n",
      "Epoch 809/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5647e-04 - val_loss: 1.2342\n",
      "Epoch 810/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.5451e-04 - val_loss: 1.2286\n",
      "Epoch 811/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.5403e-04 - val_loss: 1.2257\n",
      "Epoch 812/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5303e-04 - val_loss: 1.2323\n",
      "Epoch 813/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5180e-04 - val_loss: 1.2343\n",
      "Epoch 814/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5212e-04 - val_loss: 1.2332\n",
      "Epoch 815/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.5026e-04 - val_loss: 1.2311\n",
      "Epoch 816/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4940e-04 - val_loss: 1.2293\n",
      "Epoch 817/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4821e-04 - val_loss: 1.2372\n",
      "Epoch 818/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4759e-04 - val_loss: 1.2377\n",
      "Epoch 819/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4590e-04 - val_loss: 1.2270\n",
      "Epoch 820/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4693e-04 - val_loss: 1.2303\n",
      "Epoch 821/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4454e-04 - val_loss: 1.2337\n",
      "Epoch 822/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4423e-04 - val_loss: 1.2327\n",
      "Epoch 823/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4289e-04 - val_loss: 1.2402\n",
      "Epoch 824/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.4187e-04 - val_loss: 1.2381\n",
      "Epoch 825/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.4140e-04 - val_loss: 1.2312\n",
      "Epoch 826/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3994e-04 - val_loss: 1.2340\n",
      "Epoch 827/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3920e-04 - val_loss: 1.2347\n",
      "Epoch 828/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.3734e-04 - val_loss: 1.2392\n",
      "Epoch 829/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3739e-04 - val_loss: 1.2386\n",
      "Epoch 830/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3670e-04 - val_loss: 1.2375\n",
      "Epoch 831/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3534e-04 - val_loss: 1.2368\n",
      "Epoch 832/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3437e-04 - val_loss: 1.2354\n",
      "Epoch 833/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3364e-04 - val_loss: 1.2334\n",
      "Epoch 834/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.3394e-04 - val_loss: 1.2352\n",
      "Epoch 835/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.3251e-04 - val_loss: 1.2468\n",
      "Epoch 836/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.3096e-04 - val_loss: 1.2389\n",
      "Epoch 837/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.2955e-04 - val_loss: 1.2349\n",
      "Epoch 838/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2971e-04 - val_loss: 1.2368\n",
      "Epoch 839/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.2797e-04 - val_loss: 1.2439\n",
      "Epoch 840/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.2770e-04 - val_loss: 1.2417\n",
      "Epoch 841/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2622e-04 - val_loss: 1.2397\n",
      "Epoch 842/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2641e-04 - val_loss: 1.2457\n",
      "Epoch 843/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2508e-04 - val_loss: 1.2465\n",
      "Epoch 844/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2393e-04 - val_loss: 1.2366\n",
      "Epoch 845/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2349e-04 - val_loss: 1.2381\n",
      "Epoch 846/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2232e-04 - val_loss: 1.2455\n",
      "Epoch 847/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2144e-04 - val_loss: 1.2473\n",
      "Epoch 848/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.2181e-04 - val_loss: 1.2437\n",
      "Epoch 849/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1983e-04 - val_loss: 1.2401\n",
      "Epoch 850/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1892e-04 - val_loss: 1.2438\n",
      "Epoch 851/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.1763e-04 - val_loss: 1.2470\n",
      "Epoch 852/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1757e-04 - val_loss: 1.2449\n",
      "Epoch 853/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.1754e-04 - val_loss: 1.2344\n",
      "Epoch 854/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.1609e-04 - val_loss: 1.2457\n",
      "Epoch 855/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1502e-04 - val_loss: 1.2506\n",
      "Epoch 856/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1407e-04 - val_loss: 1.2490\n",
      "Epoch 857/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.1350e-04 - val_loss: 1.2459\n",
      "Epoch 858/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1286e-04 - val_loss: 1.2507\n",
      "Epoch 859/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.1237e-04 - val_loss: 1.2481\n",
      "Epoch 860/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.1068e-04 - val_loss: 1.2470\n",
      "Epoch 861/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.1142e-04 - val_loss: 1.2510\n",
      "Epoch 862/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.0939e-04 - val_loss: 1.2523\n",
      "Epoch 863/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0889e-04 - val_loss: 1.2456\n",
      "Epoch 864/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0781e-04 - val_loss: 1.2505\n",
      "Epoch 865/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0701e-04 - val_loss: 1.2505\n",
      "Epoch 866/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0620e-04 - val_loss: 1.2484\n",
      "Epoch 867/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0619e-04 - val_loss: 1.2541\n",
      "Epoch 868/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0491e-04 - val_loss: 1.2504\n",
      "Epoch 869/1000\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 2.0368e-04 - val_loss: 1.2464\n",
      "Epoch 870/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0333e-04 - val_loss: 1.2471\n",
      "Epoch 871/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.0210e-04 - val_loss: 1.2515\n",
      "Epoch 872/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0212e-04 - val_loss: 1.2554\n",
      "Epoch 873/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0072e-04 - val_loss: 1.2467\n",
      "Epoch 874/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.0053e-04 - val_loss: 1.2480\n",
      "Epoch 875/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9941e-04 - val_loss: 1.2578\n",
      "Epoch 876/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9884e-04 - val_loss: 1.2586\n",
      "Epoch 877/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9891e-04 - val_loss: 1.2474\n",
      "Epoch 878/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.9692e-04 - val_loss: 1.2501\n",
      "Epoch 879/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9604e-04 - val_loss: 1.2522\n",
      "Epoch 880/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.9565e-04 - val_loss: 1.2590\n",
      "Epoch 881/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9535e-04 - val_loss: 1.2537\n",
      "Epoch 882/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9397e-04 - val_loss: 1.2518\n",
      "Epoch 883/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9341e-04 - val_loss: 1.2541\n",
      "Epoch 884/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9348e-04 - val_loss: 1.2610\n",
      "Epoch 885/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9250e-04 - val_loss: 1.2524\n",
      "Epoch 886/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9169e-04 - val_loss: 1.2517\n",
      "Epoch 887/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.9065e-04 - val_loss: 1.2570\n",
      "Epoch 888/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8957e-04 - val_loss: 1.2594\n",
      "Epoch 889/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8933e-04 - val_loss: 1.2560\n",
      "Epoch 890/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8852e-04 - val_loss: 1.2578\n",
      "Epoch 891/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8795e-04 - val_loss: 1.2533\n",
      "Epoch 892/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8775e-04 - val_loss: 1.2554\n",
      "Epoch 893/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8626e-04 - val_loss: 1.2600\n",
      "Epoch 894/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.8610e-04 - val_loss: 1.2636\n",
      "Epoch 895/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8500e-04 - val_loss: 1.2609\n",
      "Epoch 896/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8453e-04 - val_loss: 1.2560\n",
      "Epoch 897/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8380e-04 - val_loss: 1.2583\n",
      "Epoch 898/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8356e-04 - val_loss: 1.2596\n",
      "Epoch 899/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8273e-04 - val_loss: 1.2566\n",
      "Epoch 900/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8176e-04 - val_loss: 1.2637\n",
      "Epoch 901/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8167e-04 - val_loss: 1.2622\n",
      "Epoch 902/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8050e-04 - val_loss: 1.2596\n",
      "Epoch 903/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.8003e-04 - val_loss: 1.2616\n",
      "Epoch 904/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7999e-04 - val_loss: 1.2660\n",
      "Epoch 905/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7847e-04 - val_loss: 1.2583\n",
      "Epoch 906/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7804e-04 - val_loss: 1.2600\n",
      "Epoch 907/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7801e-04 - val_loss: 1.2636\n",
      "Epoch 908/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7783e-04 - val_loss: 1.2662\n",
      "Epoch 909/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7598e-04 - val_loss: 1.2557\n",
      "Epoch 910/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7632e-04 - val_loss: 1.2606\n",
      "Epoch 911/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7491e-04 - val_loss: 1.2637\n",
      "Epoch 912/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7405e-04 - val_loss: 1.2670\n",
      "Epoch 913/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7372e-04 - val_loss: 1.2669\n",
      "Epoch 914/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7269e-04 - val_loss: 1.2620\n",
      "Epoch 915/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7265e-04 - val_loss: 1.2599\n",
      "Epoch 916/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7170e-04 - val_loss: 1.2691\n",
      "Epoch 917/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7156e-04 - val_loss: 1.2707\n",
      "Epoch 918/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.7056e-04 - val_loss: 1.2665\n",
      "Epoch 919/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6950e-04 - val_loss: 1.2637\n",
      "Epoch 920/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6975e-04 - val_loss: 1.2650\n",
      "Epoch 921/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6939e-04 - val_loss: 1.2721\n",
      "Epoch 922/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6807e-04 - val_loss: 1.2680\n",
      "Epoch 923/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6690e-04 - val_loss: 1.2630\n",
      "Epoch 924/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6763e-04 - val_loss: 1.2634\n",
      "Epoch 925/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6576e-04 - val_loss: 1.2710\n",
      "Epoch 926/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6620e-04 - val_loss: 1.2708\n",
      "Epoch 927/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6509e-04 - val_loss: 1.2650\n",
      "Epoch 928/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6428e-04 - val_loss: 1.2703\n",
      "Epoch 929/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6396e-04 - val_loss: 1.2729\n",
      "Epoch 930/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6317e-04 - val_loss: 1.2707\n",
      "Epoch 931/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6300e-04 - val_loss: 1.2648\n",
      "Epoch 932/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6266e-04 - val_loss: 1.2655\n",
      "Epoch 933/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6133e-04 - val_loss: 1.2717\n",
      "Epoch 934/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6102e-04 - val_loss: 1.2768\n",
      "Epoch 935/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.6068e-04 - val_loss: 1.2739\n",
      "Epoch 936/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5965e-04 - val_loss: 1.2692\n",
      "Epoch 937/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5920e-04 - val_loss: 1.2669\n",
      "Epoch 938/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5874e-04 - val_loss: 1.2707\n",
      "Epoch 939/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5842e-04 - val_loss: 1.2750\n",
      "Epoch 940/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5724e-04 - val_loss: 1.2679\n",
      "Epoch 941/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5680e-04 - val_loss: 1.2706\n",
      "Epoch 942/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.5628e-04 - val_loss: 1.2722\n",
      "Epoch 943/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5547e-04 - val_loss: 1.2728\n",
      "Epoch 944/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5475e-04 - val_loss: 1.2700\n",
      "Epoch 945/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5565e-04 - val_loss: 1.2680\n",
      "Epoch 946/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5437e-04 - val_loss: 1.2741\n",
      "Epoch 947/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5359e-04 - val_loss: 1.2809\n",
      "Epoch 948/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5340e-04 - val_loss: 1.2774\n",
      "Epoch 949/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5231e-04 - val_loss: 1.2785\n",
      "Epoch 950/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5194e-04 - val_loss: 1.2769\n",
      "Epoch 951/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5141e-04 - val_loss: 1.2755\n",
      "Epoch 952/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5089e-04 - val_loss: 1.2767\n",
      "Epoch 953/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.5014e-04 - val_loss: 1.2785\n",
      "Epoch 954/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4979e-04 - val_loss: 1.2741\n",
      "Epoch 955/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4944e-04 - val_loss: 1.2783\n",
      "Epoch 956/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4876e-04 - val_loss: 1.2780\n",
      "Epoch 957/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4797e-04 - val_loss: 1.2770\n",
      "Epoch 958/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4783e-04 - val_loss: 1.2797\n",
      "Epoch 959/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4667e-04 - val_loss: 1.2838\n",
      "Epoch 960/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4693e-04 - val_loss: 1.2812\n",
      "Epoch 961/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4655e-04 - val_loss: 1.2766\n",
      "Epoch 962/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4601e-04 - val_loss: 1.2800\n",
      "Epoch 963/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4495e-04 - val_loss: 1.2794\n",
      "Epoch 964/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4497e-04 - val_loss: 1.2837\n",
      "Epoch 965/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4516e-04 - val_loss: 1.2840\n",
      "Epoch 966/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4398e-04 - val_loss: 1.2779\n",
      "Epoch 967/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4410e-04 - val_loss: 1.2840\n",
      "Epoch 968/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4352e-04 - val_loss: 1.2773\n",
      "Epoch 969/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4258e-04 - val_loss: 1.2813\n",
      "Epoch 970/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4163e-04 - val_loss: 1.2827\n",
      "Epoch 971/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4152e-04 - val_loss: 1.2800\n",
      "Epoch 972/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.4121e-04 - val_loss: 1.2839\n",
      "Epoch 973/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4049e-04 - val_loss: 1.2790\n",
      "Epoch 974/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3954e-04 - val_loss: 1.2835\n",
      "Epoch 975/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3875e-04 - val_loss: 1.2840\n",
      "Epoch 976/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3842e-04 - val_loss: 1.2887\n",
      "Epoch 977/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3822e-04 - val_loss: 1.2828\n",
      "Epoch 978/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3749e-04 - val_loss: 1.2791\n",
      "Epoch 979/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3707e-04 - val_loss: 1.2866\n",
      "Epoch 980/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3716e-04 - val_loss: 1.2922\n",
      "Epoch 981/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3652e-04 - val_loss: 1.2847\n",
      "Epoch 982/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.3564e-04 - val_loss: 1.2842\n",
      "Epoch 983/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3528e-04 - val_loss: 1.2872\n",
      "Epoch 984/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3455e-04 - val_loss: 1.2871\n",
      "Epoch 985/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3412e-04 - val_loss: 1.2845\n",
      "Epoch 986/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3349e-04 - val_loss: 1.2858\n",
      "Epoch 987/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3308e-04 - val_loss: 1.2889\n",
      "Epoch 988/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3267e-04 - val_loss: 1.2872\n",
      "Epoch 989/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.3235e-04 - val_loss: 1.2909\n",
      "Epoch 990/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3188e-04 - val_loss: 1.2911\n",
      "Epoch 991/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3143e-04 - val_loss: 1.2850\n",
      "Epoch 992/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3082e-04 - val_loss: 1.2863\n",
      "Epoch 993/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3059e-04 - val_loss: 1.2899\n",
      "Epoch 994/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.3019e-04 - val_loss: 1.2919\n",
      "Epoch 995/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2967e-04 - val_loss: 1.2911\n",
      "Epoch 996/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.2873e-04 - val_loss: 1.2888\n",
      "Epoch 997/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.2890e-04 - val_loss: 1.2882\n",
      "Epoch 998/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.2865e-04 - val_loss: 1.2905\n",
      "Epoch 999/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2746e-04 - val_loss: 1.2949\n",
      "Epoch 1000/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2726e-04 - val_loss: 1.2956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ee0dab4e8d0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation=\"relu\",input_shape=(4096,)))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(40, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=1000, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "0.83\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "yy_pred = np.argmax(y_pred,axis=1)\n",
    "yy_test = np.argmax(y_test,axis=1)\n",
    "\n",
    "print(accuracy_score(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.50      0.60         6\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       0.75      0.75      0.75         4\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.67      1.00      0.80         4\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.25      1.00      0.40         1\n",
      "           8       1.00      1.00      1.00         3\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         1\n",
      "          11       0.67      1.00      0.80         2\n",
      "          12       0.50      0.33      0.40         3\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         3\n",
      "          15       1.00      1.00      1.00         4\n",
      "          16       1.00      1.00      1.00         2\n",
      "          17       0.50      0.50      0.50         2\n",
      "          18       1.00      1.00      1.00         1\n",
      "          19       0.67      1.00      0.80         2\n",
      "          20       0.67      0.67      0.67         3\n",
      "          21       1.00      0.67      0.80         3\n",
      "          22       1.00      1.00      1.00         2\n",
      "          23       0.75      0.75      0.75         4\n",
      "          25       1.00      1.00      1.00         3\n",
      "          26       1.00      1.00      1.00         2\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      0.33      0.50         3\n",
      "          29       1.00      1.00      1.00         1\n",
      "          30       1.00      1.00      1.00         3\n",
      "          32       1.00      1.00      1.00         4\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       0.75      0.75      0.75         4\n",
      "          36       1.00      1.00      1.00         3\n",
      "          37       1.00      1.00      1.00         2\n",
      "          39       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.83       100\n",
      "   macro avg       0.83      0.84      0.82       100\n",
      "weighted avg       0.84      0.83      0.82       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(yy_test,yy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
